[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "",
    "text": "0.1 Background and Motivation\nAs natural language processing (NLP) models are increasingly deployed in areas such as sentiment analysis and public opinion monitoring, interpretability has become a critical concern. In particular, when working with short, informal text like tweets, it is often difficult to understand the decisions made by complex black-box models.\nThis project explores the use of LIME (Local Interpretable Model-agnostic Explanations) to explain NLP model predictions and evaluates the stability and usefulness of these explanations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Explaining Text Classification Models with LIME</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.2 Research Questions",
    "text": "0.2 Research Questions\nWe focus on the following key questions:\n\nHow stable are LIME explanations for short and noisy texts like tweets?\nHow sensitive are the explanations to minor text perturbations (e.g., synonyms, punctuation)?\nCan enhanced visualizations improve human understanding of model decisions?",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Explaining Text Classification Models with LIME</span>"
    ]
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.3 Dataset",
    "text": "0.3 Dataset\nWe use the Twitter Entity Sentiment Analysis dataset from Kaggle. Each example includes a tweet, a referenced entity, and a sentiment label (Positive, Neutral, or Negative) that reflects the opinion expressed toward the entity.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Explaining Text Classification Models with LIME</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.4 Authors",
    "text": "0.4 Authors\n\nShi Jinze (js6605)\n\nMao Xiyin (xm2335)",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Explaining Text Classification Models with LIME</span>"
    ]
  },
  {
    "objectID": "index.html#date",
    "href": "index.html#date",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.5 Date",
    "text": "0.5 Date\nApril 2025",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Explaining Text Classification Models with LIME</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "",
    "text": "2.1 ğŸ“‚ Dataset Description\nThis project uses the Twitter Entity Sentiment Analysis dataset, composed of:\nEach row contains: - id: unique identifier\n- entity: the target subject\n- sentiment: one of Positive, Neutral, Negative, or Irrelevant\n- tweet: the tweet content\nğŸ¯ Task: Predict the sentiment expressed toward the entity.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#dataset-description",
    "href": "data.html#dataset-description",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "",
    "text": "twitter_training.csv: Main training dataset\n\ntwitter_validation.csv: Validation dataset",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#load-data",
    "href": "data.html#load-data",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.2 ğŸ“¥ Load Data",
    "text": "2.2 ğŸ“¥ Load Data\n\n\nCode\nimport pandas as pd\n\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\n\ntrain = pd.read_csv(\"data/twitter_training.csv\", header=None, names=col_names)\nvalid = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\n\n# Remove missing/empty tweets\ntrain = train.dropna(subset=[\"tweet\"])\ntrain = train[train[\"tweet\"].str.strip().astype(bool)]",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#basic-statistics",
    "href": "data.html#basic-statistics",
    "title": "2Â  Dataset Overview",
    "section": "2.3 Basic Statistics",
    "text": "2.3 Basic Statistics\n\n\nCode\nprint(\"Training set shape:\", train.shape)\nprint(\"Validation set shape:\", valid.shape)\nprint(train.columns)\nprint(train['sentiment'].value_counts())\n\n\nTraining set shape: (73824, 4)\nValidation set shape: (1000, 4)\nIndex(['id', 'entity', 'sentiment', 'tweet'], dtype='object')\nsentiment\nNegative      22312\nPositive      20619\nNeutral       18051\nIrrelevant    12842\nName: count, dtype: int64\n\n\n##Sample Records (Training Set)\n\n\nCode\ntrain.sample(5)[[\"tweet\", \"entity\", \"sentiment\"]]\n\n\n\n\n\n\n\n\n\ntweet\nentity\nsentiment\n\n\n\n\n13290\nWale not being on a nba 2k sound track is crim...\nNBA2K\nNegative\n\n\n33347\nis\nFortnite\nPositive\n\n\n3673\nI'm sorry I can't imagine doing a shit in this...\nCallOfDutyBlackopsColdWar\nNegative\n\n\n18035\nFlowers is played out and them guys die anyway\nPlayStation5(PS5)\nIrrelevant\n\n\n49952\nAre EA servers down? Apex Legends & FIFA playe...\nFIFA\nNegative\n\n\n\n\n\n\n\n##Check Missing Values\n\n\nCode\ntrain.isnull().sum()\n\n\nid           0\nentity       0\nsentiment    0\ntweet        0\ndtype: int64\n\n\n##Sentiment Distribution (Train vs Validation)\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nsns.countplot(data=train, x=\"sentiment\", ax=axes[0])\naxes[0].set_title(\"Training Sentiment Distribution\")\n\nsns.countplot(data=valid, x=\"sentiment\", ax=axes[1])\naxes[1].set_title(\"Validation Sentiment Distribution\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n##Summary The training set has ~74k records, and the validation set has ~1k.\n686 rows with missing or empty tweets were removed from the training set.\nThe sentiment distribution is imbalanced, with Neutral being the most common label.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "3Â  Modeling",
    "section": "",
    "text": "3.1 Objective\nWe aim to build a baseline text classification model that predicts the sentiment toward a given entity in a tweet. This sets the stage for applying LIME to explain individual predictions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#load-preprocess-data",
    "href": "modeling.html#load-preprocess-data",
    "title": "3Â  Modeling",
    "section": "3.2 Load & Preprocess Data",
    "text": "3.2 Load & Preprocess Data\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\n\ntrain = pd.read_csv(\"data/twitter_training.csv\", header=None, names=col_names)\ntrain = train.dropna(subset=[\"tweet\"])\ntrain = train[train[\"tweet\"].str.strip().astype(bool)]\nX = train[\"tweet\"]\ny = train[\"sentiment\"]\n\n\n##Baseline: Logistic Regression with TF-IDF\n\n\nCode\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nmodel = make_pipeline(\n    TfidfVectorizer(max_features=10000, ngram_range=(1,2)),\n    LogisticRegression(max_iter=1000)\n)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n  Irrelevant       0.75      0.59      0.66      2568\n    Negative       0.76      0.82      0.79      4463\n     Neutral       0.74      0.70      0.72      3610\n    Positive       0.71      0.78      0.75      4124\n\n    accuracy                           0.74     14765\n   macro avg       0.74      0.72      0.73     14765\nweighted avg       0.74      0.74      0.74     14765\n\n\n\n##Save Model & Vectorizer for LIME\n\n\nCode\nimport joblib\n\n# Save pipeline\njoblib.dump(model, \"scripts/baseline_pipeline.pkl\")\n\n\n['scripts/baseline_pipeline.pkl']\n\n\n##Summary We trained a baseline logistic regression model using TF-IDF features.\nPerformance will serve as a reference when we apply interpretation methods.\nNext: Apply LIME to explain predictions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "lime.html",
    "href": "lime.html",
    "title": "4Â  LIME Interpretation",
    "section": "",
    "text": "4.1 Goal\nUse LIME (Local Interpretable Model-agnostic Explanations) to understand why our baseline model predicts a particular sentiment for a given tweet.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>LIME Interpretation</span>"
    ]
  },
  {
    "objectID": "lime.html#load-model-and-data",
    "href": "lime.html#load-model-and-data",
    "title": "4Â  LIME Interpretation",
    "section": "4.2 Load Model and Data",
    "text": "4.2 Load Model and Data\n\n\nCode\nimport joblib\nimport pandas as pd\n\n# Load pipeline (TF-IDF + LogisticRegression)\nmodel = joblib.load(\"scripts/baseline_pipeline.pkl\")\n\n# Load data and drop missing\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\ntrain = pd.read_csv(\"data/twitter_training.csv\", header=None, names=col_names)\ntrain = train.dropna(subset=[\"tweet\"])\ntrain = train[train[\"tweet\"].str.strip().astype(bool)]\n\n\n##Select a Sample for Explanation\n\n\nCode\nimport numpy as np\n\n# Sample a random row\nsample_idx = 42  # you can change this\nexample = train.iloc[sample_idx]\nprint(\"Tweet:\", example[\"tweet\"])\nprint(\"True Sentiment:\", example[\"sentiment\"])\n\n\nTweet: Check out this epic streamer!.  \nTrue Sentiment: Neutral\n\n\n##Apply LIME\n\n\nCode\nfrom lime.lime_text import LimeTextExplainer\n\nclass_names = [\"Negative\", \"Neutral\", \"Positive\"]\nexplainer = LimeTextExplainer(class_names=class_names)\n\n# Use model's predict_proba function\ntweet_text = example[\"tweet\"]\nexplanation = explainer.explain_instance(\n    tweet_text,\n    model.predict_proba,\n    num_features=10,\n    top_labels=1\n)\n\n\n##Visualize Explanation\n\n\nCode\nfrom IPython.display import display,HTML\ndisplay(HTML(explanation.as_html()))\nwith open(\"lime_output.html\", \"w\", encoding=\"utf-8\") as f:\n    f.write(explanation.as_html())\n\n\n\n        \n        \n        \n        \n        \n        \n\n\n##Summary LIME helps identify which words contributed most to the predicted label.\nInterpretation is local and may vary with different samples.\nIn the next section, we can test stability: e.g.Â slight changes in text â†’ similar explanations?",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>LIME Interpretation</span>"
    ]
  },
  {
    "objectID": "perturbation.html",
    "href": "perturbation.html",
    "title": "5Â  perturbation",
    "section": "",
    "text": "5.1 Text Perturbation for LIME Stability\nIn this section, we evaluate how robust LIME explanations are when the input text is slightly changed.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "perturbation.html#define-perturbation-functions",
    "href": "perturbation.html#define-perturbation-functions",
    "title": "5Â  perturbation",
    "section": "5.2 1. Define Perturbation Functions",
    "text": "5.2 1. Define Perturbation Functions\n\n\nCode\nimport pandas as pd\nfrom lime.lime_text import LimeTextExplainer\nimport random\nrandom.seed(42)\nexplainer = LimeTextExplainer(class_names=[\"negative\", \"neutral\", \"positive\"])\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nimport pandas as pd\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nvalid = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\n\nX = valid[\"tweet\"]\ny = valid[\"sentiment\"]\n\nmodel = make_pipeline(TfidfVectorizer(), LogisticRegression())\nmodel.fit(X, y)\n\ndef jaccard(set1, set2):\n    return len(set1 & set2) / len(set1 | set2)\n\n\ndef random_deletion(text, p=0.1):\n    words = text.split()\n    if len(words) &lt;= 1:\n        return text\n    new_words = [word for word in words if random.random() &gt; p]\n    if len(new_words) == 0:\n        new_words = [random.choice(words)]\n    return \" \".join(new_words)\n\ndef random_insertion(text, p=0.1):\n    words = text.split()\n    new_words = words.copy()\n    for i in range(len(words)):\n        if random.random() &lt; p:\n            insert_idx = random.randint(0, len(new_words))\n            new_words.insert(insert_idx, random.choice(words))\n    return \" \".join(new_words)\n\n\n##2. Generate Perturbed Versions\n\n\nCode\nexample = valid.sample(1).iloc[0]\ntweet_text = example[\"tweet\"]\n\nnum_perturbations = 5\nperturbed_texts = []\n\nfor _ in range(num_perturbations):\n    text = random_deletion(tweet_text, p=0.2)\n    perturbed_texts.append(text)\n\n\n##Re-run LIME on Perturbed Samples\n\n\nCode\nperturbed_exps = []\n\nfor p_text in perturbed_texts:\n    p_exp = explainer.explain_instance(p_text, model.predict_proba, num_features=10, top_labels=1)\n    perturbed_exps.append(p_exp)\n\n\n##Measure Similarity (Jaccard Overlap)\n\n\nCode\n# åŸå§‹ tweet æ–‡æœ¬\ntweet_text = example[\"tweet\"]\n\n# åŸå§‹è§£é‡Šå™¨è¾“å‡º\noriginal_exp = explainer.explain_instance(\n    tweet_text,\n    model.predict_proba,\n    num_features=10,\n    top_labels=1\n)\n\n# è·å–è§£é‡Šç”¨çš„åˆ†ç±» labelï¼ˆè½¬ä¸º intï¼‰\nlabel = int(original_exp.top_labels[0])\n\n# æå–é‡è¦è¯\noriginal_words = set([w for w, _ in original_exp.as_list(label=label)])\nstability_scores = []\n\n# éå†æ¯ä¸ªæ‰°åŠ¨æ ·æœ¬çš„è§£é‡Šå¯¹è±¡\nfor exp in perturbed_exps:\n    # å¦‚æœå½“å‰è§£é‡Šå¯¹è±¡ä¸­æ²¡æœ‰è¿™ä¸ª label çš„è§£é‡Šï¼Œå°±è·³è¿‡\n    if label not in exp.available_labels():\n        continue\n    perturbed_words = set([w for w, _ in exp.as_list(label=label)])\n    score = jaccard(original_words, perturbed_words)\n    stability_scores.append(score)\n\nstability_scores\n\n\n[0.8181818181818182,\n 0.6666666666666666,\n 0.8181818181818182,\n 0.42857142857142855,\n 0.6666666666666666]\n\n\n##Summary We perturbed the input by deleting some words.\nWe evaluated how much the top LIME tokens changed (Jaccard similarity).\nNext: visualize how those changes look.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "6Â  visualization",
    "section": "",
    "text": "6.1 Visualize Stability Scores\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nstability_scores = [0.8, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8]\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=list(range(1, len(stability_scores)+1)), y=stability_scores)\nplt.xlabel(\"Perturbation Sample #\")\nplt.ylabel(\"Jaccard Similarity with Original Explanation\")\nplt.title(\"Stability of LIME Explanations under Perturbations\")\nplt.ylim(0, 1.05)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>visualization</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "7Â  Conclusion",
    "section": "",
    "text": "7.1 Project Summary\nIn this project, we explored the stability of LIME explanations for a sentiment classification task on the Twitter Entity Sentiment Analysis dataset.\nWe implemented the following key components:",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#project-summary",
    "href": "conclusion.html#project-summary",
    "title": "7Â  Conclusion",
    "section": "",
    "text": "Built a baseline text classifier to predict sentiment (Positive, Neutral, Negative) based on tweets.\nApplied LIME (Local Interpretable Model-Agnostic Explanations) to interpret model predictions at the instance level.\nGenerated perturbed versions of input texts to assess how LIME explanations change under slight modifications.\nUsed Jaccard similarity to quantify the overlap in top important words across perturbed samples.\nVisualized stability scores across perturbations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#key-findings",
    "href": "conclusion.html#key-findings",
    "title": "7Â  Conclusion",
    "section": "7.2 Key Findings",
    "text": "7.2 Key Findings\n\nLIME explanations are reasonably stable under minor perturbations (average Jaccard scores &gt; 0.8).\nImportant tokens such as named entities or sentiment-laden words tend to remain consistent across versions.\nHowever, in some cases, a small change in wording can lead to shifts in important features, especially when:\n\nThe prediction confidence is low;\nThe tweet is short or ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#limitations",
    "href": "conclusion.html#limitations",
    "title": "7Â  Conclusion",
    "section": "7.3 Limitations",
    "text": "7.3 Limitations\n\nLIME relies on bag-of-words perturbations, which may not preserve sentence fluency or semantic coherence.\nExplanations vary depending on the classifierâ€™s confidence and sensitivity to input noise.\nOur current model is relatively simple; stability might differ on deeper or more contextual models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#future-work",
    "href": "conclusion.html#future-work",
    "title": "7Â  Conclusion",
    "section": "7.4 Future Work",
    "text": "7.4 Future Work\n\nCompare LIME vs SHAP on the same dataset to assess differences in explanation quality and robustness.\nExperiment with more diverse perturbation strategies (e.g., synonym substitution, contextual masking).\nUse a more powerful model (e.g., fine-tuned BERT) and study whether model complexity improves or worsens stability.\nisualize word importance trajectories over increasing perturbation intensity.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#final-note",
    "href": "conclusion.html#final-note",
    "title": "7Â  Conclusion",
    "section": "7.5 Final Note",
    "text": "7.5 Final Note\nLIME provides valuable local interpretability, but its explanations are not always stable. Evaluating robustness under perturbations helps us understand when we can trust an explanation â€” and when we should be cautious.",
    "crumbs": [
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "data.html#dataset-dimensions",
    "href": "data.html#dataset-dimensions",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.3 ğŸ“ Dataset Dimensions",
    "text": "2.3 ğŸ“ Dataset Dimensions\n\n\nCode\nshape_df = pd.DataFrame({\n    \"Dataset\": [\"Training Set\", \"Validation Set\"],\n    \"Rows\": [train.shape[0], valid.shape[0]],\n    \"Columns\": [train.shape[1], valid.shape[1]]\n})\nshape_df\n\n\n\n\n\n\n\n\n\nDataset\nRows\nColumns\n\n\n\n\n0\nTraining Set\n73824\n4\n\n\n1\nValidation Set\n1000\n4",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#column-names",
    "href": "data.html#column-names",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.4 ğŸ“‹ Column Names",
    "text": "2.4 ğŸ“‹ Column Names\n\n\nCode\npd.DataFrame({\"Columns\": train.columns})\n\n\n\n\n\n\n\n\n\nColumns\n\n\n\n\n0\nid\n\n\n1\nentity\n\n\n2\nsentiment\n\n\n3\ntweet",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#sentiment-label-counts",
    "href": "data.html#sentiment-label-counts",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.5 ğŸ“Š Sentiment Label Counts",
    "text": "2.5 ğŸ“Š Sentiment Label Counts\n\n\nCode\nsentiment_df = train['sentiment'].value_counts().sort_values(ascending=False).reset_index()\nsentiment_df.columns = [\"Sentiment\", \"Count\"]\nsentiment_df\n\n\n\n\n\n\n\n\n\nSentiment\nCount\n\n\n\n\n0\nNegative\n22312\n\n\n1\nPositive\n20619\n\n\n2\nNeutral\n18051\n\n\n3\nIrrelevant\n12842",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#sample-records",
    "href": "data.html#sample-records",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.6 ğŸ§ª Sample Records",
    "text": "2.6 ğŸ§ª Sample Records\n\n\nCode\ntrain.sample(5)[[\"tweet\", \"entity\", \"sentiment\"]]\n\n\n\n\n\n\n\n\n\ntweet\nentity\nsentiment\n\n\n\n\n64094\n@ EAMaddenNFL servers down?\nMaddenNFL\nNegative\n\n\n35122\nmy dick is one microsoft\nMicrosoft\nNegative\n\n\n23536\nahh... copy pasting from the text google doc f...\nGoogle\nNegative\n\n\n60375\nFear is prime\nFacebook\nIrrelevant\n\n\n6873\nPpl dead be rude to me just cuz i&lt;unk&gt; a amazo...\nAmazon\nPositive",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-check",
    "href": "data.html#missing-value-check",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.7 ğŸš¨ Missing Value Check",
    "text": "2.7 ğŸš¨ Missing Value Check\n\n\nCode\nmissing_df = train.isnull().sum().reset_index()\nmissing_df.columns = [\"Column\", \"Missing Values\"]\nmissing_df\n\n\n\n\n\n\n\n\n\nColumn\nMissing Values\n\n\n\n\n0\nid\n0\n\n\n1\nentity\n0\n\n\n2\nsentiment\n0\n\n\n3\ntweet\n0",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#sentiment-distribution",
    "href": "data.html#sentiment-distribution",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.8 ğŸ“‰ Sentiment Distribution",
    "text": "2.8 ğŸ“‰ Sentiment Distribution\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"whitegrid\")\npalette = sns.color_palette(\"Set2\")\n\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\n\nsns.countplot(data=train, x=\"sentiment\", \n              order=[\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"],\n              palette=palette, ax=axes[0])\naxes[0].set_title(\"Training Sentiment Distribution\", fontsize=13)\naxes[0].set_ylabel(\"Count\")\n\nsns.countplot(data=valid, x=\"sentiment\", \n              order=[\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"],\n              palette=palette, ax=axes[1])\naxes[1].set_title(\"Validation Sentiment Distribution\", fontsize=13)\naxes[1].set_ylabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/6f/4d64zbqj5816vg7gt9y_mmwr0000gn/T/ipykernel_46852/4238378009.py:9: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=train, x=\"sentiment\",\n/var/folders/6f/4d64zbqj5816vg7gt9y_mmwr0000gn/T/ipykernel_46852/4238378009.py:9: UserWarning: The palette list has more values (8) than needed (4), which may not be intended.\n  sns.countplot(data=train, x=\"sentiment\",\n/var/folders/6f/4d64zbqj5816vg7gt9y_mmwr0000gn/T/ipykernel_46852/4238378009.py:15: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.countplot(data=valid, x=\"sentiment\",\n/var/folders/6f/4d64zbqj5816vg7gt9y_mmwr0000gn/T/ipykernel_46852/4238378009.py:15: UserWarning: The palette list has more values (8) than needed (4), which may not be intended.\n  sns.countplot(data=valid, x=\"sentiment\",",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#summary",
    "href": "data.html#summary",
    "title": "2Â  ğŸ“Š Dataset Overview",
    "section": "2.9 âœ… Summary",
    "text": "2.9 âœ… Summary\n\nThe training set has ~74,000 records; validation has ~1,000\n686 empty tweets were removed from training\nNeutral is the most common label â€” indicating an imbalanced distribution",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>ğŸ“Š Dataset Overview</span>"
    ]
  }
]