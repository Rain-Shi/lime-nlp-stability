[
  {
    "objectID": "shap_bert.html",
    "href": "shap_bert.html",
    "title": "7  SHAP Interpretability Evaluation for BERT",
    "section": "",
    "text": "8 Goal\nSystematically evaluate the interpretability of a fine-tuned BERT sentiment model using SHAP on all validation tweets: - Extract and aggregate top words by SHAP value - Visualize most influential words via word cloud - Assess explanation consistency under perturbations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SHAP Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "shap_bert.html#load-model-tokenizer-and-validation-set",
    "href": "shap_bert.html#load-model-tokenizer-and-validation-set",
    "title": "7  SHAP Interpretability Evaluation for BERT",
    "section": "8.1 1. Load Model, Tokenizer, and Validation Set",
    "text": "8.1 1. Load Model, Tokenizer, and Validation Set\n\n\nCode\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport numpy as np\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nmodel_path = \"./scripts/bert_model4\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\nmodel.to(\"cuda\")\n\nclass_names = [\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"]\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nval = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\nval = val.dropna(subset=[\"tweet\"])\nval = val[val[\"tweet\"].str.strip().astype(bool)]\nval = val[val[\"sentiment\"].isin(class_names)].reset_index(drop=True)\nval = val.sample(10, random_state=42)\n\nprint(f\"✅ Loaded {len(val)} validation tweets\")\n\n\n✅ Loaded 10 validation tweets\n\n\n#2. Define SHAP Explainer and Prediction Function\n\n\nCode\nimport shap\n\ndef shap_predict(texts):\n    if isinstance(texts, np.ndarray):\n        is_pre_tokenized = isinstance(texts[0], (list, np.ndarray))\n        texts = texts.tolist()\n    else:\n        is_pre_tokenized = isinstance(texts[0], list)\n\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n        is_split_into_words=is_pre_tokenized\n    )\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        output = model(**inputs).logits\n\n    return torch.nn.functional.softmax(output, dim=-1).cpu().numpy()\n\n\n\nmasker = shap.maskers.Text(tokenizer)\nexplainer = shap.Explainer(shap_predict, masker)\n\n\n#3. Run SHAP on Validation Set and Collect Word Importances\n\n\nCode\nimport emoji\nimport pandas as pd\n\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    cleaned = no_emoji.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n    return cleaned\n\nall_shap = []\nprint(\"Running SHAP on validation samples...\")\n\nfor idx, row in val.iterrows():\n    text = clean_text(str(row[\"tweet\"]))\n    sentiment = row[\"sentiment\"]\n    print(f\"Explaining tweet {idx+1}: {text[:50]}...\")\n\n    shap_values = explainer([text])\n    pred_label = class_names[np.argmax(shap_values.values[0].sum(axis=0))]\n\n    for word, value in zip(shap_values.data[0], shap_values.values[0][np.argmax(shap_values.values[0].sum(axis=0))]):\n        all_shap.append({\n            \"tweet\": text,\n            \"true_label\": sentiment,\n            \"pred_label\": pred_label,\n            \"word\": word,\n            \"shap_value\": value\n        })\n\ndf_shap = pd.DataFrame(all_shap)\nprint(\"SHAP explanations complete.\")\n\n\nRunning SHAP on validation samples...\nExplaining tweet 522: Remote working and an increase in cloud-based data...\nExplaining tweet 738: I actually quite like the design of the ps5. It tr...\nExplaining tweet 741: New York charges Johnson & Johnson with insurance ...\nExplaining tweet 661: Chris loves me in borderlands one and two....\nExplaining tweet 412: Check out my video! #LeagueofLegends | Captured by...\nExplaining tweet 679: Amazing deal for you!\n\nLenovo Legion Y540 9th Ge...\nExplaining tweet 627: [PS4] | Assassins Creed Syndicate First Playthroug...\nExplaining tweet 514: @EAMaddenNFL servers down?...\nExplaining tweet 860: @SpeakerPelosi this is VERY INTERESTING ...\nExplaining tweet 137: So good I had to share! Check out all the items I'...\nSHAP explanations complete.\n\n\n#4. Visualize Top Words by Mean SHAP Value\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_shap_clean = df_shap[df_shap[\"word\"].str.len() &gt; 3]\ndf_shap_clean = df_shap_clean[df_shap_clean[\"word\"].str[0].str.isalpha()]\n\ntop_words = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().sort_values(ascending=False).head(20)\n\nplt.figure(figsize=(7,4))\nsns.barplot(y=top_words.index, x=top_words.values)\nplt.title(\"Top 20 Words by Average SHAP Value\")\nplt.xlabel(\"Average SHAP Value\")\nplt.ylabel(\"Word\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#5. Word Cloud of Influential SHAP Words\n\n\nCode\nfrom wordcloud import WordCloud\n\nword_freq = df_shap.groupby(\"word\")[\"shap_value\"].mean().to_dict()\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"SHAP Word Importance Cloud\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#6. (Optional) SHAP Stability Under Synonym Perturbations\n\n\nCode\nfrom nltk.corpus import wordnet\nimport nltk\nimport random\nnltk.download(\"wordnet\")\n\ndef synonym_replace(text):\n    words = text.split()\n    new_words = []\n    for word in words:\n        syns = wordnet.synsets(word)\n        if syns and random.random() &lt; 0.2:\n            lemmas = syns[0].lemma_names()\n            if lemmas:\n                new_words.append(lemmas[0].replace(\"_\", \" \"))\n                continue\n        new_words.append(word)\n    return \" \".join(new_words)\n\nstability_scores = []\n\nfor i in range(len(val)):\n    text = val.iloc[i][\"tweet\"]\n    perturbed = synonym_replace(text)\n\n    shap_orig = explainer([text])\n    shap_pert = explainer([perturbed])\n\n    idx = np.argmax(shap_orig.values[0].sum(axis=0))\n    words_orig = set(shap_orig.data[0])\n    words_pert = set(shap_pert.data[0])\n    jaccard = len(words_orig & words_pert) / len(words_orig | words_pert)\n    stability_scores.append(jaccard)\n\nprint(f\"Average Jaccard similarity over {len(val)} perturbed explanations: {np.mean(stability_scores):.3f}\")\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\16925\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nAverage Jaccard similarity over 10 perturbed explanations: 0.876\n\n\n#Summary This notebook evaluated the interpretability of a BERT model using SHAP:\nAggregated SHAP values across all tweets\nVisualized important tokens via bar plot and word cloud\nQuantified robustness to synonym-based perturbations\nNow you can compare LIME and SHAP side-by-side for interpretability quality and stability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SHAP Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "shap_logistics.html",
    "href": "shap_logistics.html",
    "title": "8  SHAP Interpretability for Logistic Regression",
    "section": "",
    "text": "8.1 Objective\nEvaluate the interpretability of the Logistic Regression baseline model using SHAP. Visualize top contributing words and measure stability under input perturbation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SHAP Interpretability for Logistic Regression</span>"
    ]
  },
  {
    "objectID": "shap_logistics.html#load-model-and-validation-set",
    "href": "shap_logistics.html#load-model-and-validation-set",
    "title": "8  SHAP Interpretability for Logistic Regression",
    "section": "8.2 1. Load Model and Validation Set",
    "text": "8.2 1. Load Model and Validation Set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport emoji\n\nmodel = joblib.load(\"scripts/baseline_pipeline.pkl\")\nvectorizer = model.named_steps[\"tfidfvectorizer\"]\nclassifier = model.named_steps[\"logisticregression\"]\nclass_names = model.classes_\n\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nval = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\nval = val.dropna(subset=[\"tweet\"])\nval = val[val[\"tweet\"].str.strip().astype(bool)]\nval = val[val[\"sentiment\"].isin(class_names)].reset_index(drop=True)\nval = val.sample(5, random_state=42)\n\nprint(f\"✅ Loaded {len(val)} validation tweets\")\n\n\n✅ Loaded 5 validation tweets\n\n\n\n\nCode\nimport shap\n\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    return no_emoji.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n\ntexts = [clean_text(t) for t in val[\"tweet\"].tolist()]\nx_val = vectorizer.transform(texts).toarray()\nfeature_names = vectorizer.get_feature_names_out()\n\n# Initialize SHAP KernelExplainer\nbackground = x_val[:5]\nexplainer = shap.KernelExplainer(classifier.predict_proba, background)\n\n# Compute SHAP values\nall_shap = []\n\nfor i, text in enumerate(texts):\n    x = x_val[i:i+1]\n    true_label = val.iloc[i][\"sentiment\"]\n    try:\n        shap_values = explainer.shap_values(x)\n        pred_probs = classifier.predict_proba(x)[0]\n        class_idx = np.argmax(pred_probs)\n        pred_label = class_names[class_idx]\n        nonzero_indices = np.nonzero(x[0])[0]\n\n        current_shap = shap_values[class_idx][0] if isinstance(shap_values, list) else shap_values[0]\n\n        for word_idx in nonzero_indices:\n            try:\n                value = current_shap[word_idx]\n                word = feature_names[word_idx]\n                all_shap.append({\n                    \"tweet\": text,\n                    \"true_label\": true_label,\n                    \"pred_label\": pred_label,\n                    \"word\": word,\n                    \"shap_value\": value\n                })\n            except IndexError:\n                continue\n    except Exception as e:\n        print(f\"⚠️ SHAP failed for index {i}: {e}\")\n        continue\n\n# Convert to DataFrame\ndf_shap = pd.DataFrame(all_shap)\nprint(\"✅ SHAP explanation complete.\")\ndf_shap.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n✅ SHAP explanation complete.\n\n\n\n\n\n\n\n\n\ntweet\ntrue_label\npred_label\nword\nshap_value\n\n\n\n\n0\nRemote working and an increase in cloud-based ...\nPositive\nPositive\n2020\n[0.0, 0.0, 0.0, 0.0]\n\n\n1\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nattacks\n[0.0, 0.0, 0.0, 0.0]\n\n\n2\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbased\n[0.0, 0.0, 0.0, 0.0]\n\n\n3\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbreach\n[0.0, 0.0, 0.0, 0.0]\n\n\n4\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbusiness\n[0.0, 0.0, 0.0, 0.0]\n\n\n\n\n\n\n\n#2. Create SHAP Explainer using KernelExplainer (black-box)\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# Clean and filter SHAP words\ndef is_clean_word(w):\n    return (\n        isinstance(w, str) and\n        len(w) &gt;= 3 and\n        w.isalpha() and\n        w.lower() not in ENGLISH_STOP_WORDS and\n        not re.search(r\"\\\\d\", w)\n    )\n\ndf_shap[\"shap_value\"] = df_shap[\"shap_value\"].apply(lambda x: float(x[0]) if isinstance(x, (np.ndarray, list)) else float(x))\ndf_shap_clean = df_shap[df_shap[\"word\"].apply(is_clean_word)]\n\ntop_words = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().sort_values(ascending=False).head(20)\n\nplt.figure(figsize=(6, 4))\nsns.barplot(y=top_words.index, x=top_words.values)\nplt.title(\"Top 20 Words by Average SHAP Value (Cleaned)\")\nplt.xlabel(\"Average SHAP Value\")\nplt.ylabel(\"Word\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#3. Clean and Transform Text\n\n\nCode\nfrom wordcloud import WordCloud\n\nword_freq = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().to_dict()\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"SHAP Word Importance Cloud\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#4. Compute SHAP Values\n\n\nCode\nfrom nltk.corpus import wordnet\nimport nltk\nimport random\nimport sys\nimport numpy as np\nimport re\n\nnltk.download(\"wordnet\")\n\ndef safe_print(text):\n    try:\n        if isinstance(text, str):\n            encoded = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n        else:\n            encoded = str(text)\n        print(encoded)\n    except Exception as e:\n        print(f\"⚠️ Failed to print safely: {e}\", file=sys.stderr)\n\ndef strip_surrogates(text):\n    return re.sub(r'[\\ud800-\\udfff]', '', text)\n\ndef synonym_replace(text):\n    words = text.split()\n    new_words = []\n    for word in words:\n        syns = wordnet.synsets(word)\n        if syns and random.random() &lt; 0.2:\n            lemmas = syns[0].lemma_names()\n            if lemmas:\n                new_words.append(lemmas[0].replace(\"_\", \" \"))\n                continue\n        new_words.append(word)\n    return \" \".join(new_words)\n\nstability_scores = []\n\nfor i in range(len(val)):\n    raw_text = val.iloc[i][\"tweet\"]\n    text = strip_surrogates(clean_text(raw_text))          \n    perturbed = strip_surrogates(synonym_replace(text))\n\n    try:\n        \n        x_orig = vectorizer.transform([text]).toarray()\n        x_pert = vectorizer.transform([perturbed]).toarray()\n\n        shap_orig = explainer.shap_values(x_orig)\n        shap_pert = explainer.shap_values(x_pert)\n\n        class_idx = np.argmax(classifier.predict_proba(x_orig)[0])\n\n        idx_orig = np.nonzero(x_orig[0])[0]\n        idx_pert = np.nonzero(x_pert[0])[0]\n\n        words_orig = set(feature_names[idx_orig])\n        words_pert = set(feature_names[idx_pert])\n\n        \n        jaccard = len(words_orig & words_pert) / len(words_orig | words_pert)\n        stability_scores.append(jaccard)\n\n        \n        #if i &lt; 3:\n            #safe_print(f\"\\n[{i}] Original:  {text}\")\n            #safe_print(f\"[{i}] Perturbed: {perturbed}\")\n            #safe_print(f\"[{i}] Jaccard:     {jaccard:.3f}\")\n\n    except Exception as e:\n        print(f\"⚠️ SHAP failed for index {i}: {e}\")\n        continue\n\nif stability_scores:\n    safe_print(f\"\\n🔁 Average Jaccard similarity across {len(stability_scores)} samples: {np.mean(stability_scores):.3f}\")\nelse:\n    print(\"⚠️ No valid SHAP results collected.\")\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\16925\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔁 Average Jaccard similarity across 5 samples: 0.945\n\n\n#Summary We used SHAP to explain a Logistic Regression model’s decisions on validation tweets.\nTop contributing words were visualized through barplots and word clouds.\nWe measured the stability of SHAP explanations under synonym-based perturbations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SHAP Interpretability for Logistic Regression</span>"
    ]
  },
  {
    "objectID": "perturbation.html",
    "href": "perturbation.html",
    "title": "9  perturbation",
    "section": "",
    "text": "9.1 Text Perturbation for LIME Stability\nIn this section, we evaluate how robust LIME explanations are when the input text is slightly changed.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "perturbation.html#define-perturbation-functions",
    "href": "perturbation.html#define-perturbation-functions",
    "title": "9  perturbation",
    "section": "9.2 1. Define Perturbation Functions",
    "text": "9.2 1. Define Perturbation Functions\n\n\nCode\nimport pandas as pd\nfrom lime.lime_text import LimeTextExplainer\nimport random\nrandom.seed(42)\nexplainer = LimeTextExplainer(class_names=[\"negative\", \"neutral\", \"positive\"])\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nimport pandas as pd\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nvalid = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\n\nX = valid[\"tweet\"]\ny = valid[\"sentiment\"]\n\nmodel = make_pipeline(TfidfVectorizer(), LogisticRegression())\nmodel.fit(X, y)\n\ndef jaccard(set1, set2):\n    return len(set1 & set2) / len(set1 | set2)\n\n\ndef random_deletion(text, p=0.1):\n    words = text.split()\n    if len(words) &lt;= 1:\n        return text\n    new_words = [word for word in words if random.random() &gt; p]\n    if len(new_words) == 0:\n        new_words = [random.choice(words)]\n    return \" \".join(new_words)\n\ndef random_insertion(text, p=0.1):\n    words = text.split()\n    new_words = words.copy()\n    for i in range(len(words)):\n        if random.random() &lt; p:\n            insert_idx = random.randint(0, len(new_words))\n            new_words.insert(insert_idx, random.choice(words))\n    return \" \".join(new_words)\n\n\n##2. Generate Perturbed Versions\n\n\nCode\nexample = valid.sample(1).iloc[0]\ntweet_text = example[\"tweet\"]\n\nnum_perturbations = 5\nperturbed_texts = []\n\nfor _ in range(num_perturbations):\n    text = random_deletion(tweet_text, p=0.2)\n    perturbed_texts.append(text)\n\n\n##Re-run LIME on Perturbed Samples\n\n\nCode\nperturbed_exps = []\n\nfor p_text in perturbed_texts:\n    p_exp = explainer.explain_instance(p_text, model.predict_proba, num_features=10, top_labels=1)\n    perturbed_exps.append(p_exp)\n\n\n##Measure Similarity (Jaccard Overlap)\n\n\nCode\n# 原始 tweet 文本\ntweet_text = example[\"tweet\"]\n\n# 原始解释器输出\noriginal_exp = explainer.explain_instance(\n    tweet_text,\n    model.predict_proba,\n    num_features=10,\n    top_labels=1\n)\n\n# 获取解释用的分类 label（转为 int）\nlabel = int(original_exp.top_labels[0])\n\n# 提取重要词\noriginal_words = set([w for w, _ in original_exp.as_list(label=label)])\nstability_scores = []\n\n# 遍历每个扰动样本的解释对象\nfor exp in perturbed_exps:\n    # 如果当前解释对象中没有这个 label 的解释，就跳过\n    if label not in exp.available_labels():\n        continue\n    perturbed_words = set([w for w, _ in exp.as_list(label=label)])\n    score = jaccard(original_words, perturbed_words)\n    stability_scores.append(score)\n\nstability_scores\n\n\n[0.42857142857142855,\n 0.8181818181818182,\n 0.6666666666666666,\n 0.8181818181818182,\n 1.0]\n\n\n##Summary We perturbed the input by deleting some words.\nWe evaluated how much the top LIME tokens changed (Jaccard similarity).\nNext: visualize how those changes look.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "10  visualization",
    "section": "",
    "text": "10.1 Visualize Stability Scores\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nstability_scores = [0.8, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8]\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=list(range(1, len(stability_scores)+1)), y=stability_scores)\nplt.xlabel(\"Perturbation Sample #\")\nplt.ylabel(\"Jaccard Similarity with Original Explanation\")\nplt.title(\"Stability of LIME Explanations under Perturbations\")\nplt.ylim(0, 1.05)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>visualization</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "11  Conclusion",
    "section": "",
    "text": "11.1 Project Summary\nIn this project, we explored the stability of LIME explanations for a sentiment classification task on the Twitter Entity Sentiment Analysis dataset.\nWe implemented the following key components:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#project-summary",
    "href": "conclusion.html#project-summary",
    "title": "11  Conclusion",
    "section": "",
    "text": "Built a baseline text classifier to predict sentiment (Positive, Neutral, Negative) based on tweets.\nApplied LIME (Local Interpretable Model-Agnostic Explanations) to interpret model predictions at the instance level.\nGenerated perturbed versions of input texts to assess how LIME explanations change under slight modifications.\nUsed Jaccard similarity to quantify the overlap in top important words across perturbed samples.\nVisualized stability scores across perturbations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#key-findings",
    "href": "conclusion.html#key-findings",
    "title": "11  Conclusion",
    "section": "11.2 Key Findings",
    "text": "11.2 Key Findings\n\nLIME explanations are reasonably stable under minor perturbations (average Jaccard scores &gt; 0.8).\nImportant tokens such as named entities or sentiment-laden words tend to remain consistent across versions.\nHowever, in some cases, a small change in wording can lead to shifts in important features, especially when:\n\nThe prediction confidence is low;\nThe tweet is short or ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#limitations",
    "href": "conclusion.html#limitations",
    "title": "11  Conclusion",
    "section": "11.3 Limitations",
    "text": "11.3 Limitations\n\nLIME relies on bag-of-words perturbations, which may not preserve sentence fluency or semantic coherence.\nExplanations vary depending on the classifier’s confidence and sensitivity to input noise.\nOur current model is relatively simple; stability might differ on deeper or more contextual models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#future-work",
    "href": "conclusion.html#future-work",
    "title": "11  Conclusion",
    "section": "11.4 Future Work",
    "text": "11.4 Future Work\n\nCompare LIME vs SHAP on the same dataset to assess differences in explanation quality and robustness.\nExperiment with more diverse perturbation strategies (e.g., synonym substitution, contextual masking).\nUse a more powerful model (e.g., fine-tuned BERT) and study whether model complexity improves or worsens stability.\nisualize word importance trajectories over increasing perturbation intensity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#final-note",
    "href": "conclusion.html#final-note",
    "title": "11  Conclusion",
    "section": "11.5 Final Note",
    "text": "11.5 Final Note\nLIME provides valuable local interpretability, but its explanations are not always stable. Evaluating robustness under perturbations helps us understand when we can trust an explanation — and when we should be cautious.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "",
    "text": "0.1 Background and Motivation\nAs NLP models are increasingly deployed in applications such as sentiment analysis and public opinion tracking, understanding the reasoning behind their predictions has become critical—especially for short, informal, and noisy text like tweets.\nIn this project, we explore two widely used interpretability techniques—LIME and SHAP—to explain predictions made by two fundamentally different models:\nBy comparing the explanations across models and methods, we aim to uncover how model complexity affects interpretability, and whether simpler models offer more stable and human-understandable explanations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#background-and-motivation",
    "href": "index.html#background-and-motivation",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "",
    "text": "A linear model based on TF-IDF and Logistic Regression\n\nA deep learning model based on fine-tuned BERT",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.2 Research Questions",
    "text": "0.2 Research Questions\nWe seek to address the following questions:\n\nHow do LIME and SHAP differ in their explanation behavior for text classification?\nAre the explanations stable when the input is slightly perturbed?\nDo more complex models (like BERT) produce less interpretable outputs?\nHow can visualization help highlight key differences in interpretability?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#methodology-overview",
    "href": "index.html#methodology-overview",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.3 Methodology Overview",
    "text": "0.3 Methodology Overview\nOur analysis is organized into six parts:\n\nLIME Explanation for the Linear Model\n\nLIME Explanation for the BERT Model\n\nComparison of LIME Explanations across Models\n\nSHAP Explanation for the Linear Model\n\nSHAP Explanation for the BERT Model\n\nComparison of SHAP Explanations across Models\n\nEach section analyzes feature importance, visualizes word-level influence, and evaluates explanation stability under controlled perturbations.\n##️ Dataset\nWe use the Twitter Entity Sentiment Analysis dataset, which includes:\n\nA tweet\n\nA named entity mentioned in the tweet\n\nA sentiment label (Positive, Neutral, Negative, Irrelevant) reflecting public opinion",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.4 Authors",
    "text": "0.4 Authors\n\nShi Jinze (js6605)\n\nMao Xiying (xm2335)\n\n##️ Date\nApril 2025",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  }
]