[
  {
    "objectID": "shap_bert.html",
    "href": "shap_bert.html",
    "title": "7  SHAP Interpretability Evaluation for BERT",
    "section": "",
    "text": "8 Goal\nSystematically evaluate the interpretability of a fine-tuned BERT sentiment model using SHAP on all validation tweets: - Extract and aggregate top words by SHAP value - Visualize most influential words via word cloud - Assess explanation consistency under perturbations",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SHAP Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "shap_bert.html#load-model-tokenizer-and-validation-set",
    "href": "shap_bert.html#load-model-tokenizer-and-validation-set",
    "title": "7  SHAP Interpretability Evaluation for BERT",
    "section": "8.1 1. Load Model, Tokenizer, and Validation Set",
    "text": "8.1 1. Load Model, Tokenizer, and Validation Set\n\n\nCode\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport numpy as np\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nmodel_path = \"./scripts/bert_model4\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\nmodel.to(\"cuda\")\n\nclass_names = [\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"]\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nval = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\nval = val.dropna(subset=[\"tweet\"])\nval = val[val[\"tweet\"].str.strip().astype(bool)]\nval = val[val[\"sentiment\"].isin(class_names)].reset_index(drop=True)\nval = val.sample(10, random_state=42)\n\nprint(f\"✅ Loaded {len(val)} validation tweets\")\n\n\n✅ Loaded 10 validation tweets\n\n\n#2. Define SHAP Explainer and Prediction Function\n\n\nCode\nimport shap\n\ndef shap_predict(texts):\n    if isinstance(texts, np.ndarray):\n        is_pre_tokenized = isinstance(texts[0], (list, np.ndarray))\n        texts = texts.tolist()\n    else:\n        is_pre_tokenized = isinstance(texts[0], list)\n\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n        is_split_into_words=is_pre_tokenized\n    )\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        output = model(**inputs).logits\n\n    return torch.nn.functional.softmax(output, dim=-1).cpu().numpy()\n\n\n\nmasker = shap.maskers.Text(tokenizer)\nexplainer = shap.Explainer(shap_predict, masker)\n\n\n#3. Run SHAP on Validation Set and Collect Word Importances\n\n\nCode\nimport emoji\nimport pandas as pd\n\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    cleaned = no_emoji.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n    return cleaned\n\nall_shap = []\nprint(\"Running SHAP on validation samples...\")\n\nfor idx, row in val.iterrows():\n    text = clean_text(str(row[\"tweet\"]))\n    sentiment = row[\"sentiment\"]\n    print(f\"Explaining tweet {idx+1}: {text[:50]}...\")\n\n    shap_values = explainer([text])\n    pred_label = class_names[np.argmax(shap_values.values[0].sum(axis=0))]\n\n    for word, value in zip(shap_values.data[0], shap_values.values[0][np.argmax(shap_values.values[0].sum(axis=0))]):\n        all_shap.append({\n            \"tweet\": text,\n            \"true_label\": sentiment,\n            \"pred_label\": pred_label,\n            \"word\": word,\n            \"shap_value\": value\n        })\n\ndf_shap = pd.DataFrame(all_shap)\nprint(\"SHAP explanations complete.\")\n\n\nRunning SHAP on validation samples...\nExplaining tweet 522: Remote working and an increase in cloud-based data...\nExplaining tweet 738: I actually quite like the design of the ps5. It tr...\nExplaining tweet 741: New York charges Johnson & Johnson with insurance ...\nExplaining tweet 661: Chris loves me in borderlands one and two....\nExplaining tweet 412: Check out my video! #LeagueofLegends | Captured by...\nExplaining tweet 679: Amazing deal for you!\n\nLenovo Legion Y540 9th Ge...\nExplaining tweet 627: [PS4] | Assassins Creed Syndicate First Playthroug...\nExplaining tweet 514: @EAMaddenNFL servers down?...\nExplaining tweet 860: @SpeakerPelosi this is VERY INTERESTING ...\nExplaining tweet 137: So good I had to share! Check out all the items I'...\nSHAP explanations complete.\n\n\n#4. Visualize Top Words by Mean SHAP Value\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_shap_clean = df_shap[df_shap[\"word\"].str.len() &gt; 3]\ndf_shap_clean = df_shap_clean[df_shap_clean[\"word\"].str[0].str.isalpha()]\n\ntop_words = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().sort_values(ascending=False).head(20)\n\nplt.figure(figsize=(7,4))\nsns.barplot(y=top_words.index, x=top_words.values)\nplt.title(\"Top 20 Words by Average SHAP Value\")\nplt.xlabel(\"Average SHAP Value\")\nplt.ylabel(\"Word\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#5. Word Cloud of Influential SHAP Words\n\n\nCode\nfrom wordcloud import WordCloud\n\nword_freq = df_shap.groupby(\"word\")[\"shap_value\"].mean().to_dict()\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"SHAP Word Importance Cloud\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#6. (Optional) SHAP Stability Under Synonym Perturbations\n\n\nCode\nfrom nltk.corpus import wordnet\nimport nltk\nimport random\nnltk.download(\"wordnet\")\n\ndef synonym_replace(text):\n    words = text.split()\n    new_words = []\n    for word in words:\n        syns = wordnet.synsets(word)\n        if syns and random.random() &lt; 0.2:\n            lemmas = syns[0].lemma_names()\n            if lemmas:\n                new_words.append(lemmas[0].replace(\"_\", \" \"))\n                continue\n        new_words.append(word)\n    return \" \".join(new_words)\n\nstability_scores = []\n\nfor i in range(len(val)):\n    text = val.iloc[i][\"tweet\"]\n    perturbed = synonym_replace(text)\n\n    shap_orig = explainer([text])\n    shap_pert = explainer([perturbed])\n\n    idx = np.argmax(shap_orig.values[0].sum(axis=0))\n    words_orig = set(shap_orig.data[0])\n    words_pert = set(shap_pert.data[0])\n    jaccard = len(words_orig & words_pert) / len(words_orig | words_pert)\n    stability_scores.append(jaccard)\n\nprint(f\"Average Jaccard similarity over {len(val)} perturbed explanations: {np.mean(stability_scores):.3f}\")\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\16925\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nAverage Jaccard similarity over 10 perturbed explanations: 0.876\n\n\n#Summary This notebook evaluated the interpretability of a BERT model using SHAP:\nAggregated SHAP values across all tweets\nVisualized important tokens via bar plot and word cloud\nQuantified robustness to synonym-based perturbations\nNow you can compare LIME and SHAP side-by-side for interpretability quality and stability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>SHAP Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "shap_logistics.html",
    "href": "shap_logistics.html",
    "title": "8  SHAP Interpretability for Logistic Regression",
    "section": "",
    "text": "8.1 Objective\nEvaluate the interpretability of the Logistic Regression baseline model using SHAP. Visualize top contributing words and measure stability under input perturbation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SHAP Interpretability for Logistic Regression</span>"
    ]
  },
  {
    "objectID": "shap_logistics.html#load-model-and-validation-set",
    "href": "shap_logistics.html#load-model-and-validation-set",
    "title": "8  SHAP Interpretability for Logistic Regression",
    "section": "8.2 1. Load Model and Validation Set",
    "text": "8.2 1. Load Model and Validation Set\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport emoji\n\nmodel = joblib.load(\"scripts/baseline_pipeline.pkl\")\nvectorizer = model.named_steps[\"tfidfvectorizer\"]\nclassifier = model.named_steps[\"logisticregression\"]\nclass_names = model.classes_\n\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nval = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\nval = val.dropna(subset=[\"tweet\"])\nval = val[val[\"tweet\"].str.strip().astype(bool)]\nval = val[val[\"sentiment\"].isin(class_names)].reset_index(drop=True)\nval = val.sample(5, random_state=42)\n\nprint(f\"✅ Loaded {len(val)} validation tweets\")\n\n\n✅ Loaded 5 validation tweets\n\n\n\n\nCode\nimport shap\n\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    return no_emoji.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n\ntexts = [clean_text(t) for t in val[\"tweet\"].tolist()]\nx_val = vectorizer.transform(texts).toarray()\nfeature_names = vectorizer.get_feature_names_out()\n\n# Initialize SHAP KernelExplainer\nbackground = x_val[:5]\nexplainer = shap.KernelExplainer(classifier.predict_proba, background)\n\n# Compute SHAP values\nall_shap = []\n\nfor i, text in enumerate(texts):\n    x = x_val[i:i+1]\n    true_label = val.iloc[i][\"sentiment\"]\n    try:\n        shap_values = explainer.shap_values(x)\n        pred_probs = classifier.predict_proba(x)[0]\n        class_idx = np.argmax(pred_probs)\n        pred_label = class_names[class_idx]\n        nonzero_indices = np.nonzero(x[0])[0]\n\n        current_shap = shap_values[class_idx][0] if isinstance(shap_values, list) else shap_values[0]\n\n        for word_idx in nonzero_indices:\n            try:\n                value = current_shap[word_idx]\n                word = feature_names[word_idx]\n                all_shap.append({\n                    \"tweet\": text,\n                    \"true_label\": true_label,\n                    \"pred_label\": pred_label,\n                    \"word\": word,\n                    \"shap_value\": value\n                })\n            except IndexError:\n                continue\n    except Exception as e:\n        print(f\"⚠️ SHAP failed for index {i}: {e}\")\n        continue\n\n# Convert to DataFrame\ndf_shap = pd.DataFrame(all_shap)\nprint(\"✅ SHAP explanation complete.\")\ndf_shap.head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n✅ SHAP explanation complete.\n\n\n\n\n\n\n\n\n\ntweet\ntrue_label\npred_label\nword\nshap_value\n\n\n\n\n0\nRemote working and an increase in cloud-based ...\nPositive\nPositive\n2020\n[0.0, 0.0, 0.0, 0.0]\n\n\n1\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nattacks\n[0.0, 0.0, 0.0, 0.0]\n\n\n2\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbased\n[0.0, 0.0, 0.0, 0.0]\n\n\n3\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbreach\n[0.0, 0.0, 0.0, 0.0]\n\n\n4\nRemote working and an increase in cloud-based ...\nPositive\nPositive\nbusiness\n[0.0, 0.0, 0.0, 0.0]\n\n\n\n\n\n\n\n#2. Create SHAP Explainer using KernelExplainer (black-box)\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# Clean and filter SHAP words\ndef is_clean_word(w):\n    return (\n        isinstance(w, str) and\n        len(w) &gt;= 3 and\n        w.isalpha() and\n        w.lower() not in ENGLISH_STOP_WORDS and\n        not re.search(r\"\\\\d\", w)\n    )\n\ndf_shap[\"shap_value\"] = df_shap[\"shap_value\"].apply(lambda x: float(x[0]) if isinstance(x, (np.ndarray, list)) else float(x))\ndf_shap_clean = df_shap[df_shap[\"word\"].apply(is_clean_word)]\n\ntop_words = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().sort_values(ascending=False).head(20)\n\nplt.figure(figsize=(6, 4))\nsns.barplot(y=top_words.index, x=top_words.values)\nplt.title(\"Top 20 Words by Average SHAP Value (Cleaned)\")\nplt.xlabel(\"Average SHAP Value\")\nplt.ylabel(\"Word\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n#3. Clean and Transform Text\n\n\nCode\nfrom wordcloud import WordCloud\n\nword_freq = df_shap_clean.groupby(\"word\")[\"shap_value\"].mean().to_dict()\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"SHAP Word Importance Cloud\")\nplt.show()\n\n\n\n\n\n\n\n\n\n#4. Compute SHAP Values\n\n\nCode\nfrom nltk.corpus import wordnet\nimport nltk\nimport random\nimport sys\nimport numpy as np\nimport re\n\nnltk.download(\"wordnet\")\n\ndef safe_print(text):\n    try:\n        if isinstance(text, str):\n            encoded = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n        else:\n            encoded = str(text)\n        print(encoded)\n    except Exception as e:\n        print(f\"⚠️ Failed to print safely: {e}\", file=sys.stderr)\n\ndef strip_surrogates(text):\n    return re.sub(r'[\\ud800-\\udfff]', '', text)\n\ndef synonym_replace(text):\n    words = text.split()\n    new_words = []\n    for word in words:\n        syns = wordnet.synsets(word)\n        if syns and random.random() &lt; 0.2:\n            lemmas = syns[0].lemma_names()\n            if lemmas:\n                new_words.append(lemmas[0].replace(\"_\", \" \"))\n                continue\n        new_words.append(word)\n    return \" \".join(new_words)\n\nstability_scores = []\n\nfor i in range(len(val)):\n    raw_text = val.iloc[i][\"tweet\"]\n    text = strip_surrogates(clean_text(raw_text))          \n    perturbed = strip_surrogates(synonym_replace(text))\n\n    try:\n        \n        x_orig = vectorizer.transform([text]).toarray()\n        x_pert = vectorizer.transform([perturbed]).toarray()\n\n        shap_orig = explainer.shap_values(x_orig)\n        shap_pert = explainer.shap_values(x_pert)\n\n        class_idx = np.argmax(classifier.predict_proba(x_orig)[0])\n\n        idx_orig = np.nonzero(x_orig[0])[0]\n        idx_pert = np.nonzero(x_pert[0])[0]\n\n        words_orig = set(feature_names[idx_orig])\n        words_pert = set(feature_names[idx_pert])\n\n        \n        jaccard = len(words_orig & words_pert) / len(words_orig | words_pert)\n        stability_scores.append(jaccard)\n\n        \n        #if i &lt; 3:\n            #safe_print(f\"\\n[{i}] Original:  {text}\")\n            #safe_print(f\"[{i}] Perturbed: {perturbed}\")\n            #safe_print(f\"[{i}] Jaccard:     {jaccard:.3f}\")\n\n    except Exception as e:\n        print(f\"⚠️ SHAP failed for index {i}: {e}\")\n        continue\n\nif stability_scores:\n    safe_print(f\"\\n🔁 Average Jaccard similarity across {len(stability_scores)} samples: {np.mean(stability_scores):.3f}\")\nelse:\n    print(\"⚠️ No valid SHAP results collected.\")\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\16925\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔁 Average Jaccard similarity across 5 samples: 0.945\n\n\n#Summary We used SHAP to explain a Logistic Regression model’s decisions on validation tweets.\nTop contributing words were visualized through barplots and word clouds.\nWe measured the stability of SHAP explanations under synonym-based perturbations.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>SHAP Interpretability for Logistic Regression</span>"
    ]
  },
  {
    "objectID": "perturbation.html",
    "href": "perturbation.html",
    "title": "9  perturbation",
    "section": "",
    "text": "9.1 Text Perturbation for LIME Stability\nIn this section, we evaluate how robust LIME explanations are when the input text is slightly changed.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "perturbation.html#define-perturbation-functions",
    "href": "perturbation.html#define-perturbation-functions",
    "title": "9  perturbation",
    "section": "9.2 1. Define Perturbation Functions",
    "text": "9.2 1. Define Perturbation Functions\n\n\nCode\nimport pandas as pd\nfrom lime.lime_text import LimeTextExplainer\nimport random\nrandom.seed(42)\nexplainer = LimeTextExplainer(class_names=[\"negative\", \"neutral\", \"positive\"])\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\nimport pandas as pd\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nvalid = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\n\nX = valid[\"tweet\"]\ny = valid[\"sentiment\"]\n\nmodel = make_pipeline(TfidfVectorizer(), LogisticRegression())\nmodel.fit(X, y)\n\ndef jaccard(set1, set2):\n    return len(set1 & set2) / len(set1 | set2)\n\n\ndef random_deletion(text, p=0.1):\n    words = text.split()\n    if len(words) &lt;= 1:\n        return text\n    new_words = [word for word in words if random.random() &gt; p]\n    if len(new_words) == 0:\n        new_words = [random.choice(words)]\n    return \" \".join(new_words)\n\ndef random_insertion(text, p=0.1):\n    words = text.split()\n    new_words = words.copy()\n    for i in range(len(words)):\n        if random.random() &lt; p:\n            insert_idx = random.randint(0, len(new_words))\n            new_words.insert(insert_idx, random.choice(words))\n    return \" \".join(new_words)\n\n\n##2. Generate Perturbed Versions\n\n\nCode\nexample = valid.sample(1).iloc[0]\ntweet_text = example[\"tweet\"]\n\nnum_perturbations = 5\nperturbed_texts = []\n\nfor _ in range(num_perturbations):\n    text = random_deletion(tweet_text, p=0.2)\n    perturbed_texts.append(text)\n\n\n##Re-run LIME on Perturbed Samples\n\n\nCode\nperturbed_exps = []\n\nfor p_text in perturbed_texts:\n    p_exp = explainer.explain_instance(p_text, model.predict_proba, num_features=10, top_labels=1)\n    perturbed_exps.append(p_exp)\n\n\n##Measure Similarity (Jaccard Overlap)\n\n\nCode\n# 原始 tweet 文本\ntweet_text = example[\"tweet\"]\n\n# 原始解释器输出\noriginal_exp = explainer.explain_instance(\n    tweet_text,\n    model.predict_proba,\n    num_features=10,\n    top_labels=1\n)\n\n# 获取解释用的分类 label（转为 int）\nlabel = int(original_exp.top_labels[0])\n\n# 提取重要词\noriginal_words = set([w for w, _ in original_exp.as_list(label=label)])\nstability_scores = []\n\n# 遍历每个扰动样本的解释对象\nfor exp in perturbed_exps:\n    # 如果当前解释对象中没有这个 label 的解释，就跳过\n    if label not in exp.available_labels():\n        continue\n    perturbed_words = set([w for w, _ in exp.as_list(label=label)])\n    score = jaccard(original_words, perturbed_words)\n    stability_scores.append(score)\n\nstability_scores\n\n\n[0.42857142857142855,\n 0.8181818181818182,\n 0.6666666666666666,\n 0.8181818181818182,\n 1.0]\n\n\n##Summary We perturbed the input by deleting some words.\nWe evaluated how much the top LIME tokens changed (Jaccard similarity).\nNext: visualize how those changes look.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>perturbation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "10  visualization",
    "section": "",
    "text": "10.1 Visualize Stability Scores\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nstability_scores = [0.8, 0.8, 1.0, 0.8, 0.8, 1.0, 0.8]\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=list(range(1, len(stability_scores)+1)), y=stability_scores)\nplt.xlabel(\"Perturbation Sample #\")\nplt.ylabel(\"Jaccard Similarity with Original Explanation\")\nplt.title(\"Stability of LIME Explanations under Perturbations\")\nplt.ylim(0, 1.05)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>visualization</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "11  Conclusion",
    "section": "",
    "text": "11.1 Project Summary\nIn this project, we explored the stability of LIME explanations for a sentiment classification task on the Twitter Entity Sentiment Analysis dataset.\nWe implemented the following key components:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#project-summary",
    "href": "conclusion.html#project-summary",
    "title": "11  Conclusion",
    "section": "",
    "text": "Built a baseline text classifier to predict sentiment (Positive, Neutral, Negative) based on tweets.\nApplied LIME (Local Interpretable Model-Agnostic Explanations) to interpret model predictions at the instance level.\nGenerated perturbed versions of input texts to assess how LIME explanations change under slight modifications.\nUsed Jaccard similarity to quantify the overlap in top important words across perturbed samples.\nVisualized stability scores across perturbations.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#key-findings",
    "href": "conclusion.html#key-findings",
    "title": "11  Conclusion",
    "section": "11.2 Key Findings",
    "text": "11.2 Key Findings\n\nLIME explanations are reasonably stable under minor perturbations (average Jaccard scores &gt; 0.8).\nImportant tokens such as named entities or sentiment-laden words tend to remain consistent across versions.\nHowever, in some cases, a small change in wording can lead to shifts in important features, especially when:\n\nThe prediction confidence is low;\nThe tweet is short or ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#limitations",
    "href": "conclusion.html#limitations",
    "title": "11  Conclusion",
    "section": "11.3 Limitations",
    "text": "11.3 Limitations\n\nLIME relies on bag-of-words perturbations, which may not preserve sentence fluency or semantic coherence.\nExplanations vary depending on the classifier’s confidence and sensitivity to input noise.\nOur current model is relatively simple; stability might differ on deeper or more contextual models.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#future-work",
    "href": "conclusion.html#future-work",
    "title": "11  Conclusion",
    "section": "11.4 Future Work",
    "text": "11.4 Future Work\n\nCompare LIME vs SHAP on the same dataset to assess differences in explanation quality and robustness.\nExperiment with more diverse perturbation strategies (e.g., synonym substitution, contextual masking).\nUse a more powerful model (e.g., fine-tuned BERT) and study whether model complexity improves or worsens stability.\nisualize word importance trajectories over increasing perturbation intensity.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "conclusion.html#final-note",
    "href": "conclusion.html#final-note",
    "title": "11  Conclusion",
    "section": "11.5 Final Note",
    "text": "11.5 Final Note\nLIME provides valuable local interpretability, but its explanations are not always stable. Evaluating robustness under perturbations helps us understand when we can trust an explanation — and when we should be cautious.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "",
    "text": "1 Background and Motivation\nAs NLP models are increasingly deployed in applications such as sentiment analysis and public opinion tracking, understanding the reasoning behind their predictions has become critical—especially for short, informal, and noisy text like tweets.\nIn this project, we explore the interpretability of a fine-tuned BERT model—a high-performing yet opaque deep learning model—by comparing it to a much simpler, inherently interpretable linear model based on TF-IDF and Logistic Regression.\nWe apply two widely-used model explanation techniques, LIME and SHAP, to examine and contrast the reasoning processes behind these models’ predictions. Our goal is to assess whether the accuracy advantage offered by deep models like BERT comes at a significant cost to interpretability, and to what extent this trade-off manifests in real-world, noisy data. By analyzing explanation stability, word importance attribution, and human interpretability, we aim to understand the practical implications of using black-box models in socially sensitive NLP applications.\n\n\n2 Research Questions\nThis project aims to answer the following research questions:\nOur project investigates the following research questions:\n\nTo what extent do LIME and SHAP produce consistent and meaningful explanations for short, noisy texts such as tweets?\nHow does the interpretability of a simple, transparent model (TF-IDF + Logistic Regression) compare to that of a complex, high-performing model (fine-tuned BERT)?\nAre the explanations generated by each model stable under small perturbations of the input text?\nDoes the increase in predictive accuracy offered by deep models like BERT come at a significant cost to explanation quality?\nHow can visualizations support the comparison of interpretability across different models and explanation methods?\n\n\n\n3 Methodology Overview\nOur analysis is organized into six parts:\n\nLIME Explanations for the Linear Model\n\nLIME Explanations for the BERT Model\n\nComparison of LIME-Based Interpretations Across Models\n\nSHAP Explanations for the Linear Model\n\nSHAP Explanations for the BERT Model\n\nComparison of SHAP-Based Interpretations Across Models\n\nEach section examines word-level feature importance, visualizes explanation outputs, and assesses explanation robustness under input perturbations. Together, these analyses allow us to evaluate the impact of model complexity on interpretability, and compare how LIME and SHAP behave across white-box and black-box classifiers.\n#️ Dataset\nWe use the Twitter Entity Sentiment Analysis dataset, which includes:\n\nA tweet\n\nA named entity mentioned in the tweet\n\nA sentiment label (Positive, Neutral, Negative, Irrelevant) reflecting public opinion\n\n\n\n4 Authors\n\nShi Jinze (js6605)\n\nMao Xiying (xm2335)\n\n\n\n5 Date\nApril 2025",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#background-and-motivation",
    "href": "index.html#background-and-motivation",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "",
    "text": "A linear model based on TF-IDF and Logistic Regression\n\nA deep learning model based on fine-tuned BERT",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining Text Classification Models with LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "0.2 # Research Questions",
    "text": "0.2 # Research Questions\nThis project aims to answer the following research questions:\nOur project investigates the following research questions:\n\nTo what extent do LIME and SHAP produce consistent and meaningful explanations for short, noisy texts such as tweets?\nHow does the interpretability of a simple, transparent model (TF-IDF + Logistic Regression) compare to that of a complex, high-performing model (fine-tuned BERT)?\nAre the explanations generated by each model stable under small perturbations of the input text?\nDoes the increase in predictive accuracy offered by deep models like BERT come at a significant cost to explanation quality?\nHow can visualizations support the comparison of interpretability across different models and explanation methods?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#methodology-overview",
    "href": "index.html#methodology-overview",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "1.1 Methodology Overview",
    "text": "1.1 Methodology Overview\nOur analysis is organized into six parts:\n\nLIME Explanations for the Linear Model\n\nLIME Explanations for the BERT Model\n\nComparison of LIME-Based Interpretations Across Models\n\nSHAP Explanations for the Linear Model\n\nSHAP Explanations for the BERT Model\n\nComparison of SHAP-Based Interpretations Across Models\n\nEach section examines word-level feature importance, visualizes explanation outputs, and assesses explanation robustness under input perturbations. Together, these analyses allow us to evaluate the impact of model complexity on interpretability, and compare how LIME and SHAP behave across white-box and black-box classifiers.\n##️ Dataset\nWe use the Twitter Entity Sentiment Analysis dataset, which includes:\n\nA tweet\n\nA named entity mentioned in the tweet\n\nA sentiment label (Positive, Neutral, Negative, Irrelevant) reflecting public opinion",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Explaining Text Classification Models with LIME:Stability and Interpretability in NLP",
    "section": "3.1 Authors",
    "text": "3.1 Authors\n\nShi Jinze (js6605)\n\nMao Xiying (xm2335)\n\n##️ Date\nApril 2025",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Dataset Overview",
    "section": "",
    "text": "2.1 Dataset Description\nThis project uses the Twitter Entity Sentiment Analysis dataset. It consists of two files:\nEach row contains: - an ID - a target entity - the sentiment label: Positive, Neutral, Negative, or  Irrelevant - a tweet\nThe task is to predict the sentiment expressed toward the entity.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#dataset-description",
    "href": "data.html#dataset-description",
    "title": "2  Dataset Overview",
    "section": "",
    "text": "twitter_training.csv: Main training dataset\n\ntwitter_validation.csv: Validation dataset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#sample-records-training-set",
    "href": "data.html#sample-records-training-set",
    "title": "2  Dataset Overview",
    "section": "2.2 Sample Records (Training Set)",
    "text": "2.2 Sample Records (Training Set)\nWe display a few sample records from the training set to get a sense of what the tweets and associated sentiment labels look like. This is helpful for qualitative understanding of the input data before preprocessing.\n\n\nCode\nimport pandas as pd\n\n# Define column names\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\n\n# Load CSVs with no header row\ntrain = pd.read_csv(\"data/twitter_training.csv\", header=None, names=col_names)\nvalid = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\n\ntrain.sample(5)[[\"tweet\", \"entity\", \"sentiment\"]]\n\n\n\n\n\n\n\n\n\ntweet\nentity\nsentiment\n\n\n\n\n19412\nI just deserved [the Tour of Duty: Dazard]!\nWorldOfCraft\nNeutral\n\n\n44855\n[ Your boss screwed my/my friend clear of dinn...\nVerizon\nNegative\n\n\n48463\n@ AspynOvard I just saw your last vlog where y...\nHomeDepot\nNegative\n\n\n13122\n@2KSupport @NBA2K @Ronnie2K the jump shot that...\nNBA2K\nNegative\n\n\n29618\nSilly Robot. pic.nX5ZM85G\nApexLegends\nNeutral",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#data-cleaning",
    "href": "data.html#data-cleaning",
    "title": "2  Dataset Overview",
    "section": "2.3 Data Cleaning",
    "text": "2.3 Data Cleaning\nThe dataset is used to train and evaluate sentiment classification models.\n\n2.3.1 Delete Missing Data\nIn this step, we remove rows with missing or empty tweet texts to ensure clean inputs for training.\n\n\nCode\n# Remove rows with missing or empty tweets\nprint(train.isnull().sum())\n\ntrain = train.dropna(subset=[\"tweet\"])\ntrain = train[train[\"tweet\"].str.strip().astype(bool)]\n\n\nid             0\nentity         0\nsentiment      0\ntweet        686\ndtype: int64\n\n\n\n\n2.3.2 Delete Missing Emoji\nIn this step, we remove emojis from tweets to clean the text and ensure consistent tokenization for vectorization. Here is the examples:\n\n\nCode\nimport emoji\n\n# Define cleaning function\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    return no_emoji.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n\n# Apply to training set\ntrain[\"tweet\"] = train[\"tweet\"].apply(clean_text)\n\n#ourexample\nsamples = [\n    \"I'm so happy today! 😄🎉\",\n    \"Great job! 💯🔥\",\n    \"This is weird... 🤔🙃\",\n    \"Just finished my code 🐍💻\"\n]\n\nmax_len = max(len(s) for s in samples)\n\nfor s in samples:\n    cleaned = clean_text(s)\n    print(f\"{s.ljust(max_len)}  →  {cleaned}\")\n\n\nI'm so happy today! 😄🎉    →  I'm so happy today! \nGreat job! 💯🔥             →  Great job! \nThis is weird... 🤔🙃       →  This is weird... \nJust finished my code 🐍💻  →  Just finished my code",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#basic-statistics",
    "href": "data.html#basic-statistics",
    "title": "2  Dataset Overview",
    "section": "2.4 Basic Statistics",
    "text": "2.4 Basic Statistics\nWe explore the basic statistics of the dataset, including class distributions and dataset sizes. This helps us understand potential class imbalance and verify the dataset was loaded correctly.\n\n\nCode\nsentiment_counts = train[\"sentiment\"].value_counts().reset_index()\nsentiment_counts.columns = [\"Sentiment\", \"Count\"]\nsentiment_counts.index.name = \"Index\"\n\nsentiment_counts.style.set_table_styles(\n    [{\"selector\": \"th\", \"props\": [(\"text-align\", \"center\")]}]\n).set_properties(**{\n    'text-align': 'center',\n    'border': '1px solid lightgrey',\n    'background-color': '#f9f9f9'\n}).hide(axis=\"index\")\n\n\n\n\n\n\n\nSentiment\nCount\n\n\n\n\nNegative\n22312\n\n\nPositive\n20619\n\n\nNeutral\n18051\n\n\nIrrelevant\n12842\n\n\n\n\n\nWe explore the basic statistics of the dataset, including class distributions and dataset sizes. This helps us understand potential class imbalance and verify the dataset was loaded correctly.\nBased on the distribution of sentiment labels, we did not observe a significant class imbalance in the dataset.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "data.html#sentiment-distribution-bar-chart",
    "href": "data.html#sentiment-distribution-bar-chart",
    "title": "2  Dataset Overview",
    "section": "2.5 Sentiment Distribution (Bar-Chart)",
    "text": "2.5 Sentiment Distribution (Bar-Chart)\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=train, x=\"sentiment\")\nplt.title(\"Sentiment Distribution in Training Set\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"Count\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Dataset Overview</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "3  TF_IDF+Logistic Regression Modeling",
    "section": "",
    "text": "3.1 Objective\nWe aim to build a baseline text classification model that predicts the sentiment toward a given entity in a tweet.\nThe model performs 4-class classification, identifying whether the sentiment is Positive, Negative, Neutral, or Irrelevant.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TF_IDF+Logistic Regression Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#baseline-logistic-regression-with-tf-idf",
    "href": "modeling.html#baseline-logistic-regression-with-tf-idf",
    "title": "3  TF_IDF+Logistic Regression Modeling",
    "section": "3.2 Baseline: Logistic Regression with TF-IDF",
    "text": "3.2 Baseline: Logistic Regression with TF-IDF\nIn this step, we build a baseline sentiment classification model using a Scikit-learn pipeline that combines:\nTF-IDF Vectorization: Converts tweets into numeric features based on word frequency and inverse document frequency. Specifically, we:\nRemove English stopwords\nLimit vocabulary to the top 10,000 terms\nUse unigrams and bigrams (1–2-gram)\nLogistic Regression Classifier: A linear model used for multi-class classification.\nmax_iter=1000 ensures convergence for larger feature sets.\nThe dataset is split into an 80/20 train-test split. We evaluate the model using the classification_report function, which reports precision, recall, and F1-score for each sentiment class.\n\n\nCode\n# Load & Preprocess Data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\n\ntrain = pd.read_csv(\"data/twitter_training.csv\", header=None, names=col_names)\ntrain = train.dropna(subset=[\"tweet\"])\ntrain = train[train[\"tweet\"].str.strip().astype(bool)]\nX = train[\"tweet\"]\ny = train[\"sentiment\"]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nmodel = make_pipeline(\n    TfidfVectorizer(stop_words='english',max_features=10000, ngram_range=(1,2)),\n    LogisticRegression(max_iter=1000)\n)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n  Irrelevant       0.75      0.60      0.67      2568\n    Negative       0.77      0.81      0.79      4463\n     Neutral       0.75      0.70      0.72      3610\n    Positive       0.70      0.79      0.75      4124\n\n    accuracy                           0.74     14765\n   macro avg       0.74      0.73      0.73     14765\nweighted avg       0.74      0.74      0.74     14765",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TF_IDF+Logistic Regression Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#save-trained-model-for-interpretation",
    "href": "modeling.html#save-trained-model-for-interpretation",
    "title": "3  TF_IDF+Logistic Regression Modeling",
    "section": "3.3 Save Trained Model for Interpretation",
    "text": "3.3 Save Trained Model for Interpretation\nAfter training, we save the entire pipeline (including the TF-IDF vectorizer and the logistic regression classifier) using joblib. This serialized model will be used later for interpretability analysis with tools like LIME or SHAP.\n\n\nCode\nimport joblib\n\n# Save pipeline\njoblib.dump(model, \"scripts/baseline_pipeline.pkl\")\n\n\n['scripts/baseline_pipeline.pkl']",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>TF_IDF+Logistic Regression Modeling</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html",
    "href": "bert_modeling.html",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "",
    "text": "4.1 Objective\nIn this section, we fine-tune a BERT model for entity-level sentiment classification on tweets.\nThis model serves as our primary benchmark for comparison with the logistic regression baseline and for subsequent interpretability analysis using LIME.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#load-dataset-and-preprocess",
    "href": "bert_modeling.html#load-dataset-and-preprocess",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.2 Load Dataset and Preprocess",
    "text": "4.2 Load Dataset and Preprocess\nIn this section, we load the training and validation datasets, remove empty tweets, and map sentiment labels to integers: Positive → 0, Neutral → 1, Negative → 2, Irrelevant → 3. This prepares the data for model training.\n\n\nCode\nimport pandas as pd\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"data/twitter_training.csv\", header=None, names=[\"id\", \"entity\", \"sentiment\", \"tweet\"])\nval_df = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=[\"id\", \"entity\", \"sentiment\", \"tweet\"])\n\n# Ensure no missing tweets\ntrain_df = train_df.dropna(subset=[\"tweet\"])\nval_df = val_df.dropna(subset=[\"tweet\"])\n\ntrain_df = train_df[train_df[\"tweet\"].str.strip().astype(bool)]\nval_df = val_df[val_df[\"tweet\"].str.strip().astype(bool)]\n\n# Map sentiment to label\nlabel_map = {\n    \"Positive\": 0,\n    \"Neutral\": 1,\n    \"Negative\": 2,\n    \"Irrelevant\": 3\n}\n\ntrain_df[\"label\"] = train_df[\"sentiment\"].map(label_map)\nval_df[\"label\"] = val_df[\"sentiment\"].map(label_map)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#tokenization-with-huggingface",
    "href": "bert_modeling.html#tokenization-with-huggingface",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.3 Tokenization with HuggingFace",
    "text": "4.3 Tokenization with HuggingFace\nWe use Hugging Face’s AutoTokenizer to tokenize tweets into model-ready inputs, including input_ids and attention_mask. The dataset is split and preprocessed in batches using the datasets library.\n\n\nCode\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize(batch):\n    return tokenizer(batch[\"tweet\"], padding=\"max_length\", truncation=True, max_length=128)\n\nfrom datasets import Dataset\ndataset = Dataset.from_pandas(train_df[[\"tweet\", \"label\"]])\ndataset = dataset.train_test_split(test_size=0.2)\ndataset = dataset.map(tokenize, batched=True)\ndataset = dataset.map(lambda x: {\"label\": int(x[\"label\"])})\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#model-setup-bert-classification-head",
    "href": "bert_modeling.html#model-setup-bert-classification-head",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.4 Model Setup: BERT + Classification Head",
    "text": "4.4 Model Setup: BERT + Classification Head\nWe load a pretrained bert-base-uncased model with a classification head (AutoModelForSequenceClassification) to perform four-class sentiment prediction. The model is moved to GPU or CPU depending on availability.\n\n\nCode\nfrom transformers import AutoModelForSequenceClassification\nimport torch\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nBertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#training-with-trainer-api",
    "href": "bert_modeling.html#training-with-trainer-api",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.5 Training with Trainer API",
    "text": "4.5 Training with Trainer API\nThe model is fine-tuned using Hugging Face’s Trainer API, which simplifies training and evaluation by managing data loading, loss computation, gradient updates, and metric reporting.\n\n\nCode\nfrom transformers import TrainingArguments, Trainer, EvalPrediction \nfrom  sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n\n\ndef compute_metrics(pred):\n    preds = pred.predictions.argmax(axis=-1)\n    labels = pred.label_ids\n    acc = accuracy_score(labels, preds)\n    prf = precision_recall_fscore_support(labels, preds, average='macro')\n    return {\n        \"accuracy\": acc,\n        \"precision\": prf[0],\n        \"recall\": prf[1],\n        \"f1\": prf[2]\n    }\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_model\",\n    eval_strategy=\"epoch\", \n    save_strategy=\"epoch\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\"\n)\n\n\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer\n)\n\n\n#trainer.train()\n\n\nC:\\Users\\16925\\AppData\\Local\\Temp\\ipykernel_9952\\3269255061.py:35: FutureWarning:\n\n`tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#save-model",
    "href": "bert_modeling.html#save-model",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.6 Save Model",
    "text": "4.6 Save Model\n\n\nCode\n#trainer.evaluate()\n\n# Save model and tokenizer\n#trainer.save_pretrained(\"scripts/bert_model4\")           \n#tokenizer.save_pretrained(\"scripts/bert_model4\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "bert_modeling.html#evaluate-save-model",
    "href": "bert_modeling.html#evaluate-save-model",
    "title": "4  Fine-tuned BERT Model for Sentiment Classification",
    "section": "4.7 Evaluate & Save Model",
    "text": "4.7 Evaluate & Save Model\n\n\nCode\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"scripts/bert_model4\", local_files_only=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\"scripts/bert_model4\", local_files_only=True)\n\nmodel.to(\"cuda\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,           # TrainingArguments\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer\n)\n\ntrainer.evaluate()\n\n\nC:\\Users\\16925\\AppData\\Local\\Temp\\ipykernel_9952\\3729274900.py:10: FutureWarning:\n\n`tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n\n\n\n\n    \n      \n      \n      [923/923 00:18]\n    \n    \n\n\n{'eval_loss': 0.10111083835363388,\n 'eval_model_preparation_time': 0.0,\n 'eval_accuracy': 0.9682356925160853,\n 'eval_precision': 0.9689419274285296,\n 'eval_recall': 0.9677185909964447,\n 'eval_f1': 0.9682065811580038,\n 'eval_runtime': 18.4995,\n 'eval_samples_per_second': 798.13,\n 'eval_steps_per_second': 49.893}\n\n\nHere is a comparison of the BERT model vs. the Logistic Regression (TF-IDF) baseline mode\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Metrics and values\nmetrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\nlogreg_scores = [0.74, 0.74, 0.74, 0.74]\nbert_scores = [0.968, 0.969, 0.967, 0.968]\n\nx = range(len(metrics))\nbar_width = 0.35\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.bar([i - bar_width/2 for i in x], logreg_scores, width=bar_width, label=\"Logistic Regression (TF-IDF)\")\nplt.bar([i + bar_width/2 for i in x], bert_scores, width=bar_width, label=\"BERT (Fine-Tuned)\")\n\nplt.xticks(x, metrics)\nplt.ylabel(\"Score\")\nplt.ylim(0.6, 1.0)\nplt.title(\"Performance Comparison: Logistic Regression vs. BERT\")\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nKey Findings: Significant Accuracy Gain: BERT achieves a ~23% improvement in accuracy compared to the baseline, indicating better overall predictions.\nBalanced Precision and Recall: BERT maintains both high precision and recall, suggesting it not only makes correct predictions but also captures more relevant instances.\nSuperior F1-score: The improvement in F1-score demonstrates better generalization and balance between false positives and false negatives.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fine-tuned BERT Model for Sentiment Classification</span>"
    ]
  },
  {
    "objectID": "lime_bert.html",
    "href": "lime_bert.html",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "",
    "text": "5.1 Goal\nSystematically evaluate the interpretability of a fine-tuned BERT sentiment model using LIME on all validation tweets: - Extract and aggregate top words by importance - Visualize most influential words via word cloud - Assess explanation consistency under perturbations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#load-model-tokenizer-and-validation-set",
    "href": "lime_bert.html#load-model-tokenizer-and-validation-set",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.2 Load Model, Tokenizer, and Validation Set",
    "text": "5.2 Load Model, Tokenizer, and Validation Set\n\n\nCode\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Disable tokenizer warnings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Load model and tokenizer\nmodel_path = \"./scripts/bert_model4\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel.eval()\n\n# Load validation data\nclass_names = [\"Positive\", \"Neutral\", \"Negative\", \"Irrelevant\"]\ncol_names = [\"id\", \"entity\", \"sentiment\", \"tweet\"]\nval = pd.read_csv(\"data/twitter_validation.csv\", header=None, names=col_names)\nval = val.dropna(subset=[\"tweet\"])\nval = val[val[\"tweet\"].str.strip().astype(bool)]\nval = val[val[\"sentiment\"].isin(class_names)].reset_index(drop=True)\nval = val.sample(5, random_state=42)\n\nprint(f\"✅ Loaded {len(val)} validation tweets\")\n\n\n✅ Loaded 5 validation tweets",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#setup-lime-explainer-and-prediction-function",
    "href": "lime_bert.html#setup-lime-explainer-and-prediction-function",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.3 Setup LIME Explainer and Prediction Function",
    "text": "5.3 Setup LIME Explainer and Prediction Function\n\n\nCode\nfrom lime.lime_text import LimeTextExplainer\n\n# Define prediction wrapper\n\ndef bert_predict_proba(texts):\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    return probs.cpu().numpy()\n\n# Initialize LIME\nexplainer = LimeTextExplainer(class_names=class_names)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#run-lime-on-validation-set-and-collect-word-importances",
    "href": "lime_bert.html#run-lime-on-validation-set-and-collect-word-importances",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.4 Run LIME on Validation Set and Collect Word Importances",
    "text": "5.4 Run LIME on Validation Set and Collect Word Importances\n\n\nCode\nimport emoji\nimport pandas as pd\n\ndef clean_text(text):\n    no_emoji = emoji.replace_emoji(text, replace='')\n    cleaned = no_emoji.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n    return cleaned\n\nall_weights = []\nprint(f\"Starting explanation on {len(val)} tweets...\")\n\nfor idx, row in val.iterrows():\n    text = clean_text(str(row[\"tweet\"]))\n    true_sentiment = clean_text(str(row[\"sentiment\"]))\n\n    try:\n        explanation = explainer.explain_instance(\n            text_instance=text,\n            classifier_fn=bert_predict_proba,\n            num_features=10,\n            top_labels=1,\n            num_samples=100\n        )\n\n        label_idx = explanation.top_labels[0]\n        pred_label = class_names[label_idx]\n        exp_list = explanation.as_list(label=label_idx)\n\n        if not exp_list:\n            print(f\"⚠️ No explanation for tweet {idx}: {text[:30]}\")\n            continue\n\n        for word, weight in exp_list:\n            all_weights.append({\n                \"tweet\": text,\n                \"true_label\": true_sentiment,\n                \"pred_label\": clean_text(pred_label),\n                \"word\": clean_text(word),\n                \"weight\": weight\n            })\n\n    except Exception as e:\n        print(f\"❌ Error on tweet {idx}: {e}\")\n        continue\n\ndf_weights = pd.DataFrame(all_weights)\nprint(\"✅ All explanations completed.\")\n\n\nStarting explanation on 5 tweets...\n✅ All explanations completed.\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor label in class_names:\n    subset = df_weights[df_weights[\"pred_label\"] == label]\n    top_words = subset.groupby(\"word\")[\"weight\"].mean().sort_values(ascending=False).head(15)\n\n    plt.figure(figsize=(8, 5))\n    sns.barplot(y=top_words.index, x=top_words.values)\n    plt.title(f\"Top Words for Class: {label}\")\n    plt.xlabel(\"Avg Weight\")\n    plt.ylabel(\"Word\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#visualize-top-words-mean-weight",
    "href": "lime_bert.html#visualize-top-words-mean-weight",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.5 4. Visualize Top Words (Mean Weight)",
    "text": "5.5 4. Visualize Top Words (Mean Weight)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Top contributing words\ntop_words = df_weights.groupby(\"word\")[\"weight\"].mean().sort_values(ascending=False).head(20)\n\nplt.figure(figsize=(10,6))\nsns.barplot(y=top_words.index, x=top_words.values)\nplt.title(\"Top 20 Words by Average Importance\")\nplt.xlabel(\"Average Weight\")\nplt.ylabel(\"Word\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#word-cloud-of-influential-words",
    "href": "lime_bert.html#word-cloud-of-influential-words",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.6 5. Word Cloud of Influential Words",
    "text": "5.6 5. Word Cloud of Influential Words\n\n\nCode\nfrom wordcloud import WordCloud\n\nword_freq = df_weights.groupby(\"word\")[\"weight\"].mean().to_dict()\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n\nplt.figure(figsize=(12, 6))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"LIME Word Importance Cloud\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#optional-stability-evaluation-with-minor-perturbations",
    "href": "lime_bert.html#optional-stability-evaluation-with-minor-perturbations",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.7 6. (Optional) Stability Evaluation with Minor Perturbations",
    "text": "5.7 6. (Optional) Stability Evaluation with Minor Perturbations\n\n\nCode\nfrom nltk.corpus import wordnet\nimport random\nimport nltk\nnltk.download('wordnet')\n\n# Simple synonym substitution for robustness testing\ndef synonym_replace(text):\n    words = text.split()\n    new_words = []\n    for word in words:\n        syns = wordnet.synsets(word)\n        if syns and random.random() &lt; 0.2:\n            lemmas = syns[0].lemma_names()\n            if lemmas:\n                new_words.append(lemmas[0].replace(\"_\", \" \"))\n                continue\n        new_words.append(word)\n    return \" \".join(new_words)\n\n# Compare explanations between original and perturbed text\nsimilarities = []\n\nfor i in range(len(val)):\n    text = val.iloc[i][\"tweet\"]\n    perturbed = synonym_replace(text)\n\n    expl_orig = explainer.explain_instance(\n        text,\n        bert_predict_proba,\n        num_features=10,\n        top_labels=1,\n        num_samples=100\n    )\n\n    expl_pert = explainer.explain_instance(\n        perturbed,\n        bert_predict_proba,\n        num_features=10,\n        top_labels=1,\n        num_samples=100\n    )\n\n    words_orig = set(w for w, _ in expl_orig.as_list(label=expl_orig.top_labels[0]))\n    words_pert = set(w for w, _ in expl_pert.as_list(label=expl_pert.top_labels[0]))\n\n    jaccard = len(words_orig & words_pert) / len(words_orig | words_pert)\n    similarities.append(jaccard)\n\nprint(f\"Average Jaccard similarity over {len(val)} samples: {np.mean(similarities):.3f}\")\n\n\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\16925\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nAverage Jaccard similarity over 5 samples: 0.456",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  },
  {
    "objectID": "lime_bert.html#summary",
    "href": "lime_bert.html#summary",
    "title": "5  LIME Interpretability Evaluation for BERT",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThis notebook evaluated the interpretability of a BERT model on a validation set using LIME: - Aggregated word importance from all tweets - Visualized dominant features via barplot and word cloud - Measured stability of explanations under synonym substitution\nThis approach provides a quantitative view of how and why your model makes predictions. You can now include these results in your interpretability report or presentation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>LIME Interpretability Evaluation for BERT</span>"
    ]
  }
]