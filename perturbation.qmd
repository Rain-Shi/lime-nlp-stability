---
title: "Perturbation Stability: Comparing LIME and SHAP for Logistic Regression and BERT"
format: html
jupyter: python3
---

## Objectice

Synthesize findings from applying **LIME** and **SHAP** to explain two models:
- **TF-IDF + Logistic Regression**
- **Fine-tuned BERT**

### Evaluation Criteria:
- **Interpretability**: Are the word-level explanations intuitive?  
- **Stability**: Do explanations remain consistent after perturbations? (Jaccard similarity)  
- **Visualization Effectiveness**: Are the insights clearly communicated?

---

### Logistic Regression + LIME

- **Top Words**: Literal, generic terms like `"SpeakerPelosi"`, `"Montage"`, `"kid"`, `"AWESOME"`.
- **Interpretability**: âœ… High â€” linear model yields transparent explanations.
- **Stability**: **Jaccard = 0.662**
- **Insights**:
  - Words map clearly to model predictions.
  - Explanation quality is stable enough for reliable human inspection.

---

### Logistic Regression + SHAP

- **Top Words**: Similar to LIME â€” lexical, interpretable tokens with strong polarity or topic focus.
- **Interpretability**: âœ…âœ… Very High â€” SHAP's additive structure mirrors linear coefficients.
- **Stability**: **Jaccard = 0.910** ðŸ”
- **Insights**:
  - Most robust and faithful explanations among all configurations.
  - Ideal for explainable AI deployments in real-world settings.

---

### BERT + LIME

- **Top Words**: Emotionally or contextually rich words (e.g., `"inadequate"`, `"LOVE"`, `"smartrobotics"`).
- **Interpretability**: âš ï¸ Moderate â€” sometimes insightful, but inconsistent.
- **Stability**: **Jaccard = 0.475**
- **Insights**:
  - BERTâ€™s contextual embeddings are difficult to fully capture with LIMEâ€™s local linear approximations.
  - Visuals (bar plots, word clouds) help, but the sensitivity to input changes undermines reliability.

---

### BERT + SHAP

- **Top Words**: Named entities, sentiment tokens, and syntactic markers (e.g., `"Ranked"`, `"RAVENS"`, `"sad"`).
- **Interpretability**: âœ… High â€” SHAP can extract meaningful attributions from a complex black-box.
- **Stability**: **Jaccard = 0.885**
- **Insights**:
  - Despite BERTâ€™s complexity, SHAP shows strong robustness.
  - Offers a balance between deep model capacity and explanation reliability.

---

## Comparative Table

| Method | Model               | Interpretability       | Jaccard Similarity | Explanation Stability |
|--------|---------------------|------------------------|--------------------|------------------------|
| **LIME**   | Logistic Regression | High               | **0.662**          | Medium-High             |
| LIME   | BERT                |  Moderate            | 0.475              | Low                    |
| **SHAP**   | Logistic Regression | Very High        | **0.910**        | Excellent               |
| SHAP   | BERT                | High                | 0.885              | Very Good              |

---

## Takeaways

- **SHAP + Logistic Regression** provides the **best trade-off** between transparency and robustness â€” recommended for trustworthy NLP systems.
- **LIME + BERT** produces intuitive but **unstable** explanations â€” useful for local debugging, but not for auditing.
- **SHAP outperforms LIME** on both models in **stability**.
- Even for complex models like BERT, **SHAP maintains explanation consistency**, making it suitable for high-stakes applications.

> **Recommendation**: For interpretable and dependable NLP systems, favor **SHAP with simpler models** or **combine SHAP+LIME** to gain both global and local insights.
