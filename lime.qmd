---
title: "LIME Interpretation"
format: html
jupyter: python3
---

## Goal

Use **LIME (Local Interpretable Model-agnostic Explanations)** to understand **why** our baseline model predicts a particular sentiment for a given tweet.

---

## Load Model and Data

```{python}
import joblib
import pandas as pd

# Load pipeline (TF-IDF + LogisticRegression)
model = joblib.load("scripts/baseline_pipeline.pkl")

# Load data and drop missing
col_names = ["id", "entity", "sentiment", "tweet"]
train = pd.read_csv("data/twitter_training.csv", header=None, names=col_names)
train = train.dropna(subset=["tweet"])
train = train[train["tweet"].str.strip().astype(bool)]
```

##Select a Sample for Explanation
```{python}
import numpy as np

# Sample a random row
sample_idx = 42  # you can change this
example = train.iloc[sample_idx]
print("Tweet:", example["tweet"])
print("True Sentiment:", example["sentiment"])

```

##Apply LIME
```{python}
from lime.lime_text import LimeTextExplainer

class_names = ["Negative", "Neutral", "Positive"]
explainer = LimeTextExplainer(class_names=class_names)

# Use model's predict_proba function
tweet_text = example["tweet"]
explanation = explainer.explain_instance(
    tweet_text,
    model.predict_proba,
    num_features=10,
    top_labels=1
)
```

##Visualize Explanation
```{python}
from IPython.display import display,HTML
display(HTML(explanation.as_html()))
with open("lime_output.html", "w", encoding="utf-8") as f:
    f.write(explanation.as_html())
```
##Summary
LIME helps identify which words contributed most to the predicted label.

Interpretation is local and may vary with different samples.

In the next section, we can test stability: e.g. slight changes in text â†’ similar explanations?