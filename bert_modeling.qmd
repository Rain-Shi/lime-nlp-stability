---
title: "bert_modeling"
format: html
freeze: auto
---

---
title: "BERT Modeling"
format: html
jupyter: python3
---

## Objective

We fine-tune a **BERT model** for entity-level sentiment classification on tweets.

This serves as the **primary model** to compare with our baseline and explain using LIME.

---

## Load Dataset and Preprocess

```{python}
import pandas as pd
import pandas as pd

train_df = pd.read_csv("data/twitter_training.csv", header=None, names=["id", "entity", "sentiment", "tweet"])
val_df = pd.read_csv("data/twitter_validation.csv", header=None, names=["id", "entity", "sentiment", "tweet"])

# Ensure no missing tweets
train_df = train_df.dropna(subset=["tweet"])
val_df = val_df.dropna(subset=["tweet"])

train_df = train_df[train_df["tweet"].str.strip().astype(bool)]
val_df = val_df[val_df["tweet"].str.strip().astype(bool)]

# Map sentiment to label
label_map = {
    "Positive": 0,
    "Neutral": 1,
    "Negative": 2,
    "Irrelevant": 3
}

train_df["label"] = train_df["sentiment"].map(label_map)
val_df["label"] = val_df["sentiment"].map(label_map)
```


##Tokenization with HuggingFace
```{python}
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["tweet"], padding="max_length", truncation=True, max_length=128)

from datasets import Dataset
dataset = Dataset.from_pandas(train_df[["tweet", "label"]])
dataset = dataset.train_test_split(test_size=0.2)
dataset = dataset.map(tokenize, batched=True)
dataset = dataset.map(lambda x: {"label": int(x["label"])})
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])


```

##Model Setup: BERT + Classification Head
```{python}
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=4)
```

##Training with Trainer API
```{python}
from transformers import TrainingArguments, Trainer, EvalPrediction, accuracy_score
from sklearn.metrics import classification_report

# 评估指标
def compute_metrics(pred):
    preds = pred.predictions.argmax(axis=-1)
    labels = pred.label_ids
    acc = accuracy_score(labels, preds)
    prf = precision_recall_fscore_support(labels, preds, average='macro')
    return {
        "accuracy": acc,
        "precision": prf[0],
        "recall": prf[1],
        "f1": prf[2]
    }

# 训练参数
training_args = TrainingArguments(
    output_dir="./bert_model",
    eval_strategy="epoch",   # ✅ 每轮评估一次（必须配合 load_best_model_at_end 使用）
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)


# Trainer 实例化
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)

# 启动训练
trainer.train()




```

##Evaluate & Save Model
```{python}
trainer.evaluate()

# Save model and tokenizer
model.save_pretrained("scripts/bert_model4")
tokenizer.save_pretrained("scripts/bert_model4")
```

##Summary
In this experiment, we fine-tuned a BERT-based model for entity-level sentiment classification on a cleaned Twitter dataset. The original tweets were labeled as Positive, Neutral, or Negative, and the model was trained to distinguish these categories with high accuracy.

We preprocessed the data, tokenized it using HuggingFace's bert-base-uncased tokenizer, and applied the Trainer API for training and evaluation. The training was conducted for 3 epochs with batch size = 16, and the best-performing model was automatically saved using load_best_model_at_end=True.

After training, the model achieved an evaluation accuracy of 94.1%, with strong F1-scores across all sentiment classes (F1_positive ≈ 0.935, F1_neutral ≈ 0.941, F1_negative ≈ 0.948), indicating excellent generalization to unseen data.

This BERT model will be used in downstream experiments involving LIME to assess and compare interpretability and stability, benchmarked against a traditional TF-IDF + Logistic Regression baseline.