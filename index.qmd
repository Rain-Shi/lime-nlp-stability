---
title: "Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP"
format: html
---

## Background and Motivation

As **NLP models** are increasingly deployed in applications such as **sentiment analysis** and public opinion tracking, understanding the reasoning behind their predictions has become critical—especially for short, informal, and noisy text like tweets.

In this project, we explore the **interpretability** of a **fine-tuned BERT model**—a high-performing yet opaque deep learning model—by comparing it to a much simpler, inherently **interpretable linear model** based on **TF-IDF** and **Logistic Regression**.

We apply two widely-used model explanation techniques, **LIME** and **SHAP**, to examine and contrast the reasoning processes behind these models' predictions. Our goal is to assess whether the **accuracy advantage** offered by deep models like BERT comes at a significant cost to interpretability, and to what extent this **trade-off** manifests in real-world, noisy data. By analyzing explanation stability, word importance attribution, and human interpretability, we aim to understand the practical implications of using **black-box models** in socially sensitive NLP applications.

## Research Questions

This project aims to answer the following research questions:

Our project investigates the following research questions:

-   To what extent do LIME and SHAP produce consistent and meaningful explanations for short, noisy texts such as tweets?
-   How does the interpretability of a simple, transparent model (TF-IDF + Logistic Regression) compare to that of a complex, high-performing model (fine-tuned BERT)?
-   Are the explanations generated by each model stable under small perturbations of the input text?
-   Does the increase in predictive accuracy offered by deep models like BERT come at a significant cost to explanation quality?
-   How can visualizations support the comparison of interpretability across different models and explanation methods?

## Methodology Overview

Our analysis is organized into six components:

1. **LIME Interpretability for Logistic Regression**  
   Apply LIME to a transparent TF-IDF + Logistic Regression model to analyze token-level contributions and explanation clarity.

2. **LIME Interpretability for BERT**  
   Use LIME on a fine-tuned BERT model to assess how local linear approximations capture deep contextual semantics.

3. **SHAP Interpretability for Logistic Regression**  
   Apply SHAP to the logistic model to examine additive, class-agnostic explanations aligned with feature weights.

4. **SHAP Interpretability for BERT**  
   Use SHAP to interpret BERT predictions and evaluate the quality and granularity of token attributions.

5. **Perturbation Stability Analysis**  
   Introduce controlled text perturbations (e.g., synonym replacement) and compute Jaccard similarity to assess explanation robustness.

6. **Comparative Evaluation Across Models and Methods**  
   Compare LIME vs. SHAP and BERT vs. Logistic Regression in terms of interpretability, stability, and visualization effectiveness.

Each section examines word-level feature importance, visualizes key attributions (via bar plots and word clouds), and tests the reliability of explanations under realistic perturbations. Together, these components enable us to evaluate the interpretability-performance trade-off in modern NLP pipelines.



## Dataset

We use the [**Twitter Entity Sentiment Analysis** dataset](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis), which includes:

-   A tweet\
-   A named entity mentioned in the tweet\
-   A sentiment label (**Positive**, **Neutral**, **Negative, Irrelevant**) reflecting public opinion

This is an **entity-level sentiment analysis** dataset. Given a message and an entity, the task is to classify the **sentiment of the message about the entity**.
Messages that are not relevant to the entity (i.e., Irrelevant) are treated as **Neutral** in our implementation

## Authors

-   **Shi Jinze** (js6605)\
-   **Mao Xiying** (xm2335)

## Date

April 2025
