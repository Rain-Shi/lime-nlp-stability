---
title: "Explaining the Black Box: A Comparative Study of BERT and Logistic Regression using LIME and SHAP"
format: html
---

## Background and Motivation

As **NLP models** are increasingly deployed in applications such as **sentiment analysis** and public opinion tracking, understanding the reasoning behind their predictions has become critical—especially for short, informal, and noisy text like tweets.

In this project, we explore the **interpretability** of a **fine-tuned BERT model**—a high-performing yet opaque deep learning model—by comparing it to a much simpler, inherently **interpretable linear model** based on **TF-IDF** and **Logistic Regression**.

We apply two widely-used model explanation techniques, **LIME** and **SHAP**, to examine and contrast the reasoning processes behind these models' predictions. Our goal is to assess whether the **accuracy advantage** offered by deep models like BERT comes at a significant cost to interpretability, and to what extent this **trade-off** manifests in real-world, noisy data. By analyzing explanation stability, word importance attribution, and human interpretability, we aim to understand the practical implications of using **black-box models** in socially sensitive NLP applications.

## Research Questions

This project aims to answer the following research questions:

Our project investigates the following research questions:

-   To what extent do LIME and SHAP produce consistent and meaningful explanations for short, noisy texts such as tweets?
-   How does the interpretability of a simple, transparent model (TF-IDF + Logistic Regression) compare to that of a complex, high-performing model (fine-tuned BERT)?
-   Are the explanations generated by each model stable under small perturbations of the input text?
-   Does the increase in predictive accuracy offered by deep models like BERT come at a significant cost to explanation quality?
-   How can visualizations support the comparison of interpretability across different models and explanation methods?

## Methodology Overview

Our analysis is organized into six parts:

1.  **LIME Explanations for the Linear Model**\
2.  **LIME Explanations for the BERT Model**\
3.  **Comparison of LIME-Based Interpretations Across Models**\
4.  **SHAP Explanations for the Linear Model**\
5.  **SHAP Explanations for the BERT Model**\
6.  **Comparison of SHAP-Based Interpretations Across Models**

Each section examines word-level feature importance, visualizes explanation outputs, and assesses explanation robustness under input perturbations. Together, these analyses allow us to evaluate the impact of model complexity on interpretability, and compare how LIME and SHAP behave across white-box and black-box classifiers.

#️# Dataset

We use the [**Twitter Entity Sentiment Analysis** dataset](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis), which includes:

-   A tweet\
-   A named entity mentioned in the tweet\
-   A sentiment label (**Positive**, **Neutral**, **Negative, Irrelevant**) reflecting public opinion

## Authors

-   **Shi Jinze** (js6605)\
-   **Mao Xiying** (xm2335)

## Date

April 2025
